last,name,affiliation,tex,account,data_preparation,feature_selection,learning_algorithm,model_selection,other
Adem,Muna Adem,"Department of Sociology, Indiana University, Bloomington, IN","\item Muna Adem, Department of Sociology, Indiana University, Bloomington, IN",IU_Sociology,"Mean/median/mode imputation, Variable standardization/rescaling, One-hot encoding, Adding missingness indicators, Inferring feature types (e.g. categorical or continuous), Constructing your own train/test split","Using prior expertise, Reading study documentation, Conducting a literature review, Using constructed variables, Model-based feature selection (e.g. F-test or LASSO), Excluded 'constructed scales' with more than 30% missing values You're","Random forest, Data-based hyperparameter selection (e.g. cross validation)","MSE performance in training set, MSE performance on leaderboard, Inspecting regression coefficients, Interpreting hyperparameters",
Alhajri,Abdulla Alhajri,"Nuclear Science and Engineering, Massachusetts Institute of Technology, Cambridge, MA","\item Abdulla Alhajri, Nuclear Science and Engineering, Massachusetts Institute of Technology, Cambridge, MA",alhajri,"Mean/median/mode imputation, Model-based imputation, Imputation based on survey structure (e.g. skips), Dropping low-variance features","Conducting a literature review, Using constructed variables","Random forest, Gradient-boosted trees, Elastic net","MSE performance in training set, MSE performance on leaderboard",
AlShebli,Bedoor AlShebli,"New York University, Abu Dhabi","\item Bedoor AlShebli, New York University, Abu Dhabi",Anahit_Sargsyan,"Model-based imputation, Adding missingness indicators, Dropping low-variance features, KNN imputation algorithm","Model-based feature selection (e.g. F-test or LASSO), Extra Trees Regression Algorithm, Randomized Lasso","Linear regression/OLS, Logistic regression/logit, Decision tree, Random forest","MSE performance in training set, MSE performance on leaderboard, Assessing groups/clusters of features",
Amin,Redwane Amin,"Bendheim Center for Finance, Princeton University, Princeton, NJ 08544, USA","\item Redwane Amin, Bendheim Center for Finance, Princeton University, Princeton, NJ 08544, USA",spike_slab_team,"Mean/median/mode imputation, Model-based imputation, Inferring feature types (e.g. categorical or continuous), Constructing your own train/test split","Using prior expertise, Reading study documentation, Conducting a literature review, Random Forest using Gini score","Linear regression/OLS, Logistic regression/logit, Random forest, Ridge regression, LASSO, Elastic net, Data-based hyperparameter selection (e.g. cross validation), Kernel methods (e.g. kernel ridge regression or support vector machines), Spike and slab using Gnet","MSE performance in training set, MSE performance on leaderboard, Inspecting regression coefficients, Interpreting hyperparameters",
Amos,Ryan B Amos,"Department of Computer Science, Princeton University, Princeton, NJ, 08540, USA","\item Ryan B Amos, Department of Computer Science, Princeton University, Princeton, NJ, 08540, USA",rbamos,"Mean/median/mode imputation, Dropping low-variance features, Constructing your own train/test split, Feature selection with with k-means clustering",Model-based feature selection (e.g. F-test or LASSO),"Linear regression/OLS, Logistic regression/logit, Ridge regression, LASSO, Elastic net, Support Vector Machine",MSE performance in training set,
Baer-Bositis,Livia Baer-Bositis,"Department of Sociology, Stanford University, Stanford, CA 94305, USA","\item Livia Baer-Bositis, Department of Sociology, Stanford University, Stanford, CA 94305, USA",lbb285,Variable standardization/rescaling,"Conducting a literature review, Using constructed variables","Linear regression/OLS, Logistic regression/logit",Inspecting regression coefficients,none
Becker,Joshua Becker,"Kellogg School of Management, Northwestern University, Evanston, IL 60208, USA","\item Joshua Becker, Kellogg School of Management, Northwestern University, Evanston, IL 60208, USA",joshua_becker_upenn,Model-based imputation,"Using prior expertise, Conducting a literature review","Linear regression/OLS, Logistic regression/logit",skip b/c i don't remember for sure,
BÙchi,Moritz BÙchi,"Department of Communication and Media Research, University of Zurich, ZÙrich, Switzerland","\item Moritz BÙchi, Department of Communication and Media Research, University of Zurich, ZÙrich, Switzerland",mdb,Model-based imputation,"Using prior expertise, Using constructed variables",Linear regression/OLS,"MSE performance in training set, MSE performance on leaderboard, Inspecting regression coefficients",I selected only cases that were complete on all the constructed variables
Chung,Bo-Ryehn Chung,"Center for Statistics & Machine Learning, Princeton University, Princeton, NJ 08544, USA ","\item Bo-Ryehn Chung, Center for Statistics & Machine Learning, Princeton University, Princeton, NJ 08544, USA ",fw,"Mean/median/mode imputation, Variable standardization/rescaling, Inferring feature types (e.g. categorical or continuous), Dropping low-variance features","Reading study documentation, Conducting a literature review, Model-based feature selection (e.g. F-test or LASSO)","Logistic regression/logit, Ridge regression, LASSO, Elastic net, Data-based hyperparameter selection (e.g. cross validation)","MSE performance in training set, MSE performance on leaderboard, Inspecting regression coefficients",
DEMIRAG BURAK,Elif Gizem DEMIRAG BURAK,"Department of Psychology, Koç University, Istanbul, Turkey","\item Elif Gizem DEMIRAG BURAK, Department of Psychology, Koç University, Istanbul, Turkey",gturunc16,"Model-based imputation, Variable standardization/rescaling, Inferring feature types (e.g. categorical or continuous), Dropping low-variance features","Using prior expertise, Reading study documentation, Conducting a literature review, Using constructed variables","Linear regression/OLS, Logistic regression/logit",Inspecting regression coefficients,
Dora,Begum Dora,"Department of Psychology, Koc University, Istanbul, Turkey","\item Begum Dora, Department of Psychology, Koc University, Istanbul, Turkey",gturunc16,"Model-based imputation, Variable standardization/rescaling, Inferring feature types (e.g. categorical or continuous), Dropping low-variance features","Using prior expertise, Reading study documentation, Conducting a literature review, Using constructed variables","Linear regression/OLS, Logistic regression/logit",Inspecting regression coefficients,
Eggert,William Eggert,"Department of Mechanical and Aerospace Engineering, Princeton University, Princeton, NJ, 08544","\item William Eggert, Department of Mechanical and Aerospace Engineering, Princeton University, Princeton, NJ, 08544",weggert,"Imputation based on survey structure (e.g. skips), Variable standardization/rescaling, Creating synthetic features with PCA, Constructing your own train/test split","Reading study documentation, Model-based feature selection (e.g. F-test or LASSO)","Decision tree, Random forest, Gradient-boosted trees, Ridge regression, Data-based hyperparameter selection (e.g. cross validation), Manual hyperparameter setting, Kernel methods (e.g. kernel ridge regression or support vector machines)","MSE performance in training set, Inspecting regression coefficients, Assessing groups/clusters of features, Interpreting hyperparameters","Exploring a Gaussian Mixture Model Process looked like it would have produced quite accurate results (better than what I achieved), but there was not enough time to delve into it fully."
Faletto,Gregory Faletto,"Department of Data Sciences and Operations, Marshall School of Business, University of Southern California","\item Gregory Faletto, Department of Data Sciences and Operations, Marshall School of Business, University of Southern California",greg.faletto,"Model-based imputation, Variable standardization/rescaling, One-hot encoding, Creating synthetic features with PCA, Inferring feature types (e.g. categorical or continuous), Dropping low-variance features","Reading study documentation, Using constructed variables, Model-based feature selection (e.g. F-test or LASSO)","Linear regression/OLS, Random forest, LASSO, Data-based hyperparameter selection (e.g. cross validation)",MSE performance in training set,
Fan,Zhilin Fan,"a4 Media & Data Solutions, Long Island City, NY, 11120, USA","\item Zhilin Fan, a4 Media & Data Solutions, Long Island City, NY, 11120, USA",ADSgrp5,"Mean/median/mode imputation, One-hot encoding, Adding missingness indicators, Constructing your own train/test split","Using prior expertise, Using constructed variables, Mutual information","Gradient-boosted trees, Data-based hyperparameter selection (e.g. cross validation)","MSE performance on leaderboard, MSE performance in split test set",K-Means and bagging
Freese,Jeremy Freese,"Department of Sociology, Stanford University","\item Jeremy Freese, Department of Sociology, Stanford University",jeremyfreese,"Functional form transformation (e.g. logging or top-coding), Adding missingness indicators, Using multiple analytic datasets to generate predictions","Using prior expertise, Reading study documentation","Linear regression/OLS, Logistic regression/logit","MSE performance in training set, Inspecting regression coefficients",
Gadgil,Tejomay Gadgil,"Center for Data Science, NYU, New York, NY, 10011, USA","\item Tejomay Gadgil, Center for Data Science, NYU, New York, NY, 10011, USA",mdrc,"Mean/median/mode imputation, Imputation based on survey structure (e.g. skips), Variable standardization/rescaling, One-hot encoding, Dropping low-variance features","Using prior expertise, Reading study documentation, Conducting a literature review, Using constructed variables","Linear regression/OLS, Decision tree, Random forest","MSE performance in training set, Inspecting regression coefficients, Measures of variable importance (e.g. marginal effects)",
Gagn_,Josh Gagn_,"Department of Sociology, Stanford University, Stanford, CA 94305, USA","\item Josh Gagn_, Department of Sociology, Stanford University, Stanford, CA 94305, USA",jgagne,"Mean/median/mode imputation, Adding missingness indicators","Using prior expertise, Reading study documentation","Linear regression/OLS, Logistic regression/logit","MSE performance on leaderboard, Inspecting regression coefficients",I shrunk the predictions toward the training set mean
Gao,Yue Gao,Columbia University,"\item Yue Gao, Columbia University",aurora1994,"Mean/median/mode imputation, Variable standardization/rescaling, Adding missingness indicators, Inferring feature types (e.g. categorical or continuous), Dropping low-variance features, Constructing your own train/test split","Model-based feature selection (e.g. F-test or LASSO), random forest based feature selection","Random forest, Gradient-boosted trees, Data-based hyperparameter selection (e.g. cross validation)","MSE performance in training set, MSE performance on leaderboard, Assessing groups/clusters of features, Measures of variable importance (e.g. marginal effects)",
Gupta,Meera Gupta,"Department of Chemical and Biological Engineering, Princeton University, Princeton, NJ, 08544,USA","\item Meera Gupta, Department of Chemical and Biological Engineering, Princeton University, Princeton, NJ, 08544,USA",amagann,"Mean/median/mode imputation, Variable standardization/rescaling, Dropping low-variance features","Model-based feature selection (e.g. F-test or LASSO), : in certain studies, feature selection was performed by modeling only the city-related features, to test if these features alone could be predictive of GPA","Linear regression/OLS, Decision tree, Ridge regression, LASSO, Elastic net, Data-based hyperparameter selection (e.g. cross validation), Kernel methods (e.g. kernel ridge regression or support vector machines), bagging regression, support vector regression, gaussian processes. ",MSE performance in training set,
Halpern-Manners,Andrew Halpern-Manners,"Department of Sociology, Indiana University, Bloomington, IN 47405, USA","\item Andrew Halpern-Manners, Department of Sociology, Indiana University, Bloomington, IN 47405, USA",IU_Sociology,"Mean/median/mode imputation, Variable standardization/rescaling, Adding missingness indicators, Inferring feature types (e.g. categorical or continuous), Constructing your own train/test split, Dropping variables with more than 70% missing values","Using prior expertise, Reading study documentation, Conducting a literature review, Using constructed variables, Model-based feature selection (e.g. F-test or LASSO)","Random forest, LASSO, Data-based hyperparameter selection (e.g. cross validation)","MSE performance in training set, MSE performance on leaderboard, Inspecting regression coefficients, Interpreting hyperparameters",
Hashim,Sonia P. Hashim,"Department of Computer Science, University of California Santa Barbara,  Santa Barbara, CA, 93106, USA","\item Sonia P. Hashim, Department of Computer Science, University of California Santa Barbara,  Santa Barbara, CA, 93106, USA",shashim,"Mean/median/mode imputation, Model-based imputation, Imputation based on survey structure (e.g. skips), Adding missingness indicators, Dropping low-variance features","Reading study documentation, Using constructed variables, Model-based feature selection (e.g. F-test or LASSO)","Linear regression/OLS, Decision tree, Random forest, Ridge regression, LASSO, Data-based hyperparameter selection (e.g. cross validation)","MSE performance in training set, MSE performance on leaderboard",
Hausen,Sonia A. Hausen,"Department of Sociology, Stanford University, Stanford CA 94305","\item Sonia A. Hausen, Department of Sociology, Stanford University, Stanford CA 94305",shausen,"Mean/median/mode imputation, Model-based imputation, Adding missingness indicators, Inferring feature types (e.g. categorical or continuous)","Using prior expertise, Reading study documentation, Using constructed variables","Linear regression/OLS, Logistic regression/logit",MSE performance on leaderboard,
He,Guanhua He,"Department of Molecular Biology, Princeton University, Princeton, NJ, 08544, USA","\item Guanhua He, Department of Molecular Biology, Princeton University, Princeton, NJ, 08544, USA",rbamos,"Mean/median/mode imputation, Creating synthetic features with PCA, Dropping low-variance features, Constructing your own train/test split","Conducting a literature review, Model-based feature selection (e.g. F-test or LASSO)","Linear regression/OLS, Logistic regression/logit, Ridge regression, LASSO, Elastic net, Data-based hyperparameter selection (e.g. cross validation), Kernel methods (e.g. kernel ridge regression or support vector machines)","MSE performance in training set, MSE performance on leaderboard, Assessing groups/clusters of features, Interpreting hyperparameters",
Higuera,Kimberly Higuera,"Department of Sociology, Stanford University, Stanford, CA 94305 USA","\item Kimberly Higuera, Department of Sociology, Stanford University, Stanford, CA 94305 USA",khiguera,"Mean/median/mode imputation, Inferring feature types (e.g. categorical or continuous)","Using prior expertise, Reading study documentation, Using constructed variables","Linear regression/OLS, Logistic regression/logit","Inspecting regression coefficients, Interpreting hyperparameters",
Hogan,Bernie Hogan,"Oxford Internet Institute, University of Oxford, Oxford, UK, OX13JS","\item Bernie Hogan, Oxford Internet Institute, University of Oxford, Oxford, UK, OX13JS",bz24,"Creating synthetic features with PCA, Please see response by bz247[ Bingyu Zhao <bz247@cam.ac.uk> ]","Using prior expertise, Reading study documentation, Using constructed variables, Please see response by bz247[ Bingyu Zhao <bz247@cam.ac.uk> ]","Logistic regression/logit, Random forest, Please see response by bz247 [ Bingyu Zhao <bz247@cam.ac.uk> ]",Please see response by bz247[ Bingyu Zhao <bz247@cam.ac.uk> ],
Horwitz,Ilana M. Horwitz,Stanford University,"\item Ilana M. Horwitz, Stanford University",ihorwitz,Variable standardization/rescaling,"Using prior expertise, Using constructed variables","Linear regression/OLS, Logistic regression/logit","MSE performance on leaderboard, Inspecting regression coefficients",
Hummel,Lisa M. Hummel,"Department of Sociology, Stanford University, Stanford, CA 94305, USA","\item Lisa M. Hummel, Department of Sociology, Stanford University, Stanford, CA 94305, USA",Bumblebee2023,Variable standardization/rescaling,"Using prior expertise, Reading study documentation, Using constructed variables","Linear regression/OLS, Logistic regression/logit",Inspecting regression coefficients,
Jain,Naman Jain,"Department of Operations Research and Financial Engineering, Princeton University, Princeton, NJ 08544, USA","\item Naman Jain, Department of Operations Research and Financial Engineering, Princeton University, Princeton, NJ 08544, USA",amusse,Mean/median/mode imputation,Model-based feature selection (e.g. F-test or LASSO),"Linear regression/OLS, Logistic regression/logit, Decision tree, Random forest, Gradient-boosted trees, Bayesian additive regression trees (BART), Ridge regression, LASSO, Elastic net","MSE performance in training set, MSE performance on leaderboard, Inspecting regression coefficients",
Jin,Kun Jin,"Department of Computer Science, Ohio State University, Columbus, OH 43210, USA","\item Kun Jin, Department of Computer Science, Ohio State University, Columbus, OH 43210, USA",aprilfeifei,"Mean/median/mode imputation, Variable standardization/rescaling, Inferring feature types (e.g. categorical or continuous), Dropping low-variance features, Constructing your own train/test split","Using prior expertise, Model-based feature selection (e.g. F-test or LASSO)","Linear regression/OLS, LASSO, Elastic net, Data-based hyperparameter selection (e.g. cross validation)","MSE performance in training set, Inspecting regression coefficients, Interpreting hyperparameters",
Jurgens,David Jurgens,"School of Information, University of Michigan, Ann Arbor MI, 48104, USA","\item David Jurgens, School of Information, University of Michigan, Ann Arbor MI, 48104, USA",davidj,"One-hot encoding, Constructing your own train/test split","Using prior expertise, Model-based feature selection (e.g. F-test or LASSO)","Random forest, Data-based hyperparameter selection (e.g. cross validation), Manual hyperparameter setting","MSE performance in training set, MSE performance on leaderboard","There were many failed attempts at designing models (bad performance on the training data) before submitting the final model.  I have checked options above that were done during the whole design process, which are not necessarily reflected in the final submitted code."
Kaban,Enes Kaban,"MA Candidate, Department of Psychology, Koc University, Istanbul, 34450, Turkey","\item Enes Kaban, MA Candidate, Department of Psychology, Koc University, Istanbul, 34450, Turkey",ekaban15,"Variable standardization/rescaling, Inferring feature types (e.g. categorical or continuous), Multiple Imputation","Reading study documentation, Model-based feature selection (e.g. F-test or LASSO)","Linear regression/OLS, Logistic regression/logit",Measures of variable importance (e.g. marginal effects),
Kaminski,Patrick C. Kaminski,"Department of Sociology & Center for Complex Networks and Systems Research, Indiana University Bloomington, Bloomington, IN 47405, USA","\item Patrick C. Kaminski, Department of Sociology & Center for Complex Networks and Systems Research, Indiana University Bloomington, Bloomington, IN 47405, USA",IU_Sociology,"Mean/median/mode imputation, Variable standardization/rescaling, One-hot encoding, Adding missingness indicators, Inferring feature types (e.g. categorical or continuous), Constructing your own train/test split, Excluded 'constructed scales' with more than 30% missing values","Using prior expertise, Reading study documentation, Conducting a literature review, Using constructed variables, Model-based feature selection (e.g. F-test or LASSO)","Random forest, Data-based hyperparameter selection (e.g. cross validation)","MSE performance in training set, MSE performance on leaderboard, Inspecting regression coefficients, Interpreting hyperparameters",
Kanter,"Jeremy B. Kanter, Ph.D.","Department of Family and Consumer Sciences, Illinois State University, Normal, IL, 61761","\item Jeremy B. Kanter, Ph.D., Department of Family and Consumer Sciences, Illinois State University, Normal, IL, 61761",jeremykanter,"Variable standardization/rescaling, Adding missingness indicators, Inferring feature types (e.g. categorical or continuous)","Using prior expertise, Reading study documentation, Conducting a literature review, Using constructed variables","Linear regression/OLS, Logistic regression/logit","MSE performance in training set, Inspecting regression coefficients, Assessing groups/clusters of features",
Karapetyan,Areg Karapetyan,"1. Department of Computer Science, Masdar Institute, Khalifa University, Abu Dhabi, UAE  2. Research Institute for Mathematical Sciences (RIMS) Kyoto University, Kyoto 606- 8502, Japan","\item Areg Karapetyan, 1. Department of Computer Science, Masdar Institute, Khalifa University, Abu Dhabi, UAE  2. Research Institute for Mathematical Sciences (RIMS) Kyoto University, Kyoto 606- 8502, Japan",Anahit_Sargsyan,"Model-based imputation, Adding missingness indicators, Dropping low-variance features, kNN (k-Nearest Neighbors) imputation algorithm","Model-based feature selection (e.g. F-test or LASSO), Extra Trees Regressor algorithm, Randomized Lasso","Linear regression/OLS, Logistic regression/logit, Decision tree, Random forest","MSE performance in training set, MSE performance on leaderboard, Assessing groups/clusters of features",
Kim,E. H. Kim,"Department of Sociology, Stanford University, Stanford, CA 94305, USA","\item E. H. Kim, Department of Sociology, Stanford University, Stanford, CA 94305, USA",ehk02004,"Variable standardization/rescaling, Creating synthetic features with PCA","Using prior expertise, Reading study documentation, Using constructed variables","Linear regression/OLS, Logistic regression/logit",Inspecting regression coefficients,
Leizman,Ben Leizman,"Department of Computer Science, Princeton University, Princeton, NJ 08544, USA","\item Ben Leizman, Department of Computer Science, Princeton University, Princeton, NJ 08544, USA",bleizman,"Mean/median/mode imputation, Constructing your own train/test split","Conducting a literature review, Mutual information","Ridge regression, LASSO, Elastic net, Neural networks, K-Nearest Neighbors","MSE performance in training set, MSE performance on leaderboard, Inspecting regression coefficients",
Liu,David Liu,"Department of Computer Science, Princeton University, Princeton, NJ 08544, USA.","\item David Liu, Department of Computer Science, Princeton University, Princeton, NJ 08544, USA.",dliu18,Adding missingness indicators,"None of the above. In my final submission, I predicted the mean so I did not utilize the features.","I used neural networks and manual hyperparameter setting in my experiments, but for the final submission, I did not use any of the above.","MSE performance in training set, MSE performance on leaderboard",
Liu,Naijia Liu,"Department of Politics, Princeton University, Princeton NJ 08544, USA","\item Naijia Liu, Department of Politics, Princeton University, Princeton NJ 08544, USA",NaijiaLiu,"Imputation based on survey structure (e.g. skips), Constructing your own train/test split","Using prior expertise, Reading study documentation","Linear regression/OLS, Ridge regression, Neural networks","MSE performance in training set, MSE performance on leaderboard",
Mack,Andrew E. Mack,"Department of Politics, Princeton University, Princeton, NJ 08544, USA.","\item Andrew E. Mack, Department of Politics, Princeton University, Princeton, NJ 08544, USA.",aemack,"Mean/median/mode imputation, Inferring feature types (e.g. categorical or continuous), Constructing your own train/test split",Model-based feature selection (e.g. F-test or LASSO),"Random forest, Gradient-boosted trees, LASSO, Data-based hyperparameter selection (e.g. cross validation)",MSE performance in training set,
Magann,Alicia Magann,"Department of Chemical and Biological Engineering, Princeton University, Princeton, NJ 08544, USA","\item Alicia Magann, Department of Chemical and Biological Engineering, Princeton University, Princeton, NJ 08544, USA",amagann,"Mean/median/mode imputation, Variable standardization/rescaling, Dropping low-variance features","Model-based feature selection (e.g. F-test or LASSO), in certain studies, feature selection was performed by modeling only the city-related features, to test if these features alone could be predictive of GPA.","Linear regression/OLS, Decision tree, Ridge regression, LASSO, Elastic net, Data-based hyperparameter selection (e.g. cross validation), Kernel methods (e.g. kernel ridge regression or support vector machines), bagging regression, support vector regression, gaussian processes.",MSE performance in training set,
Mahajan,Mayank Mahajan,"Department of Computer Science, Princeton University, Princeton, NJ 08544, USA","\item Mayank Mahajan, Department of Computer Science, Princeton University, Princeton, NJ 08544, USA",mahajanm,"Mean/median/mode imputation, Model-based imputation, Variable standardization/rescaling, Dropping low-variance features, Constructing your own train/test split","Using prior expertise, Conducting a literature review, Model-based feature selection (e.g. F-test or LASSO)","Linear regression/OLS, Logistic regression/logit, Gradient-boosted trees, Ridge regression, LASSO, Elastic net, Data-based hyperparameter selection (e.g. cross validation)","MSE performance in training set, MSE performance on leaderboard, Measures of variable importance (e.g. marginal effects)",
Mandell,Noah Mandell,"Department of Astrophysical Sciences, Princeton University, Princeton, NJ 08544, USA","\item Noah Mandell, Department of Astrophysical Sciences, Princeton University, Princeton, NJ 08544, USA",nmandell,"Mean/median/mode imputation, One-hot encoding, Adding missingness indicators, Inferring feature types (e.g. categorical or continuous), Dropping low-variance features, dropping columns containing strings",No feature selection,"Linear regression/OLS, Random forest, Ridge regression, Data-based hyperparameter selection (e.g. cross validation), Huber regression","MSE performance in training set, MSE performance on leaderboard, Inspecting regression coefficients",
Marahrens,Helge-Johannes Marahrens,"Department of Sociology, Indiana University, Bloomington, IN 47405, USA","\item Helge-Johannes Marahrens, Department of Sociology, Indiana University, Bloomington, IN 47405, USA",IU_Sociology,"Mean/median/mode imputation, Variable standardization/rescaling, One-hot encoding, Adding missingness indicators, Inferring feature types (e.g. categorical or continuous), Constructing your own train/test split","Using prior expertise, Reading study documentation, Conducting a literature review, Using constructed variables, Model-based feature selection (e.g. F-test or LASSO), Excluded 'constructed scales' with more than 30% missing values","Random forest, Data-based hyperparameter selection (e.g. cross validation)","MSE performance in training set, MSE performance on leaderboard, Inspecting regression coefficients, Interpreting hyperparameters",
Melin,Julia L. Melin,"Department of Sociology, Stanford University, Stanford, CA 94305, USA","\item Julia L. Melin, Department of Sociology, Stanford University, Stanford, CA 94305, USA",jmels13,Mean/median/mode imputation,"Using prior expertise, Reading study documentation",Linear regression/OLS,Inspecting regression coefficients,
Mercado-Garcia,Diana Mercado-Garcia,"Graduate School of Education, Stanford University, CA ","\item Diana Mercado-Garcia, Graduate School of Education, Stanford University, CA ",ddmmgg,"Mean/median/mode imputation, Model-based imputation","Using prior expertise, Reading study documentation, Using constructed variables, Model-based feature selection (e.g. F-test or LASSO)","Linear regression/OLS, Logistic regression/logit","Inspecting regression coefficients, Assessing groups/clusters of features",
Mocz,Viola Mocz,"Department of Neuroscience, Princeton University, Princeton, NJ 08544, USA","\item Viola Mocz, Department of Neuroscience, Princeton University, Princeton, NJ 08544, USA",shashim,"Mean/median/mode imputation, Model-based imputation, Imputation based on survey structure (e.g. skips), Adding missingness indicators, Dropping low-variance features","Reading study documentation, Using constructed variables, Model-based feature selection (e.g. F-test or LASSO)","Linear regression/OLS, Decision tree, Random forest, Ridge regression, LASSO, Data-based hyperparameter selection (e.g. cross validation)","MSE performance in training set, MSE performance on leaderboard",
M_ser,Malte M_ser,"Department of Computer Science, Princeton University, Princeton, NJ 08544, USA","\item Malte M_ser, Department of Computer Science, Princeton University, Princeton, NJ 08544, USA",malte,"Mean/median/mode imputation, Variable standardization/rescaling, One-hot encoding, Adding missingness indicators, Inferring feature types (e.g. categorical or continuous), Dropping low-variance features",Model-based feature selection (e.g. F-test or LASSO),"Elastic net, Data-based hyperparameter selection (e.g. cross validation)","MSE performance in training set, MSE performance on leaderboard",
Motazedi,Ehsan Motazedi,"Plant sciences group, Wageningen university & research, The Netherlands","\item Ehsan Motazedi, Plant sciences group, Wageningen university & research, The Netherlands",redpotato,Imputation based on survey structure (e.g. skips),Model-based feature selection (e.g. F-test or LASSO),"Linear regression/OLS, Logistic regression/logit, Elastic net","MSE performance in training set, MSE performance on leaderboard, Inspecting regression coefficients",We used forward and backward selection method to choose the best features for prediction using AIC.
Mueller-Gastell,Katariina Mueller-Gastell,"Department of Sociology, Stanford University, Stanford, CA 94305, USA","\item Katariina Mueller-Gastell, Department of Sociology, Stanford University, Stanford, CA 94305, USA",katamg,"Variable standardization/rescaling, One-hot encoding, Functional form transformation (e.g. logging or top-coding), Adding missingness indicators, Inferring feature types (e.g. categorical or continuous), Dropping low-variance features, Constructing your own train/test split","Reading study documentation, Conducting a literature review, Using constructed variables, Leveraging time structure and repeated measures (e.g. lagged DVs), Model-based feature selection (e.g. F-test or LASSO)","Linear regression/OLS, Logistic regression/logit","MSE performance in training set, MSE performance on leaderboard, Inspecting regression coefficients",
Musse,Ahmed Musse,"Department of Electrical Engineering, Princeton University, Princeton, NJ, 08544, USA","\item Ahmed Musse, Department of Electrical Engineering, Princeton University, Princeton, NJ, 08544, USA",amusse,Mean/median/mode imputation,Select K Best using chi square statistics of each feature,"Logistic regression/logit, Decision tree, Random forest, Gradient-boosted trees, Ridge regression, LASSO, Data-based hyperparameter selection (e.g. cross validation), Kernel methods (e.g. kernel ridge regression or support vector machines), Bagging, K-Nearest Neighbors, Gaussian Process Regression","Assessing groups/clusters of features, Interpreting hyperparameters",
Niu,Qiankun Niu,Princeton University,"\item Qiankun Niu, Princeton University",hty,"Mean/median/mode imputation, Imputation based on survey structure (e.g. skips), Dropping low-variance features","Conducting a literature review, Model-based feature selection (e.g. F-test or LASSO)","Linear regression/OLS, Logistic regression/logit, Random forest, Ridge regression, LASSO, Elastic net, Data-based hyperparameter selection (e.g. cross validation)","MSE performance in training set, Interpreting hyperparameters, Measures of variable importance (e.g. marginal effects)",
Nowak,William P. Nowak,Unbox Research,"\item William P. Nowak, Unbox Research",wnowak,"Model-based imputation, Using multiple analytic datasets to generate predictions",Model-based feature selection (e.g. F-test or LASSO),Gradient-boosted trees,Measures of variable importance (e.g. marginal effects),
Omidvar,Hamidreza Omidvar,"Department of Civil and Environmental Engineering, Princeton University, Princeton, Nj, 08544, USA","\item Hamidreza Omidvar, Department of Civil and Environmental Engineering, Princeton University, Princeton, Nj, 08544, USA",hamidrezaomidvar,"Mean/median/mode imputation, Model-based imputation","Model-based feature selection (e.g. F-test or LASSO), Correlation based selection","Linear regression/OLS, Logistic regression/logit, Ridge regression, LASSO",MSE performance in training set,
Or,Andrew Or,"Department of Computer Science, Princeton University, Princeton, NJ, 08544, USA","\item Andrew Or, Department of Computer Science, Princeton University, Princeton, NJ, 08544, USA",t.f.schaffner,"Mean/median/mode imputation, Dropping low-variance features","Mutual information, Model-based feature selection (e.g. F-test or LASSO)","Linear regression/OLS, Random forest, Gradient-boosted trees, LASSO, Kernel methods (e.g. kernel ridge regression or support vector machines), Naive bayes, k-nearest neighbors","MSE performance in training set, MSE performance on leaderboard, Inspecting regression coefficients",No
Ouyang,Karen Ouyang,Princeton '17,"\item Karen Ouyang, Princeton '17",kouyang,"Mean/median/mode imputation, One-hot encoding, Inferring feature types (e.g. categorical or continuous), Using multiple analytic datasets to generate predictions, Constructing your own train/test split","Using prior expertise, Reading study documentation, Mutual information, Model-based feature selection (e.g. F-test or LASSO)","Linear regression/OLS, Ridge regression, LASSO, Elastic net, Data-based hyperparameter selection (e.g. cross validation), Kernel methods (e.g. kernel ridge regression or support vector machines)","MSE performance in training set, MSE performance on leaderboard, Inspecting regression coefficients, Assessing groups/clusters of features",
Ézer,Demet Ézer,"Department of Psychology, Koç University, Istanbul, Turkey","\item Demet Ézer, Department of Psychology, Koç University, Istanbul, Turkey",gturunc16,"Model-based imputation, Variable standardization/rescaling, Inferring feature types (e.g. categorical or continuous), Dropping low-variance features","Using prior expertise, Reading study documentation, Conducting a literature review, Using constructed variables","Linear regression/OLS, Logistic regression/logit",Inspecting regression coefficients,
Pinto,Katy M. Pinto,"Department of Sociology, CSU, Dominguez Hills, CA 90747","\item Katy M. Pinto, Department of Sociology, CSU, Dominguez Hills, CA 90747",Katy_P,"Mean/median/mode imputation, Model-based imputation, Imputation based on survey structure (e.g. skips), Adding missingness indicators","Using prior expertise, Reading study documentation, Conducting a literature review, Using constructed variables","Linear regression/OLS, Logistic regression/logit","MSE performance in training set, MSE performance on leaderboard, Inspecting regression coefficients",
Porter,Ethan Porter,"School of Media and Public Affairs, George Washington University, Washington D.C., 20052","\item Ethan Porter, School of Media and Public Affairs, George Washington University, Washington D.C., 20052",lennyc,Inferring feature types (e.g. categorical or continuous),"Using prior expertise, Reading study documentation",Linear regression/OLS,Inspecting regression coefficients,
Porter,Kristin E. Porter,MDRC,"\item Kristin E. Porter, MDRC",mdrc,"Adding missingness indicators, Inferring feature types (e.g. categorical or continuous), Dropping low-variance features, Constructing your own train/test split","Using prior expertise, Reading study documentation, Using constructed variables, Model-based feature selection (e.g. F-test or LASSO)","Logistic regression/logit, Random forest, Gradient-boosted trees, Ridge regression, LASSO, Elastic net, Data-based hyperparameter selection (e.g. cross validation), Kernel methods (e.g. kernel ridge regression or support vector machines), Ensembling",MSE performance in training set,
Qian,Crystal Qian,"Department of Computer Science, Princeton University, Princeton, NJ, 08544","\item Crystal Qian, Department of Computer Science, Princeton University, Princeton, NJ, 08544",cjqian,"Mean/median/mode imputation, Model-based imputation, One-hot encoding, Adding missingness indicators, Constructing your own train/test split",Model-based feature selection (e.g. F-test or LASSO),"Ridge regression, LASSO, Elastic net, Data-based hyperparameter selection (e.g. cross validation), Manual hyperparameter setting, Least angle regression","MSE performance in training set, MSE performance on leaderboard, Interpreting hyperparameters",
Rauf,Tamkinat Rauf,"Department of Sociology, Stanford University, Stanford, CA 94305, USA","\item Tamkinat Rauf, Department of Sociology, Stanford University, Stanford, CA 94305, USA",Tamkinat,"Mean/median/mode imputation, Imputation based on survey structure (e.g. skips)","Conducting a literature review, Using constructed variables","Linear regression/OLS, Logistic regression/logit","MSE performance in training set, MSE performance on leaderboard, Inspecting regression coefficients",
Rodriguez,"Maria Y. Rodriguez, PhD, MSW","SIlberman School of Social Work, Hunter College (CUNY) 2180 Third Ave New York, NY 10035","\item Maria Y. Rodriguez, PhD, MSW, SIlberman School of Social Work, Hunter College (CUNY) 2180 Third Ave New York, NY 10035",maria_yr,Mean/median/mode imputation,"Using prior expertise, Reading study documentation, Using constructed variables",Logistic regression/logit,"MSE performance in training set, MSE performance on leaderboard, Inspecting regression coefficients",
Russell,Luke T. Russell,"Department of Family and Consumer Sciences, Illinois State University, Normal, IL 61790, USA.","\item Luke T. Russell, Department of Family and Consumer Sciences, Illinois State University, Normal, IL 61790, USA.",jeremykanter,"Dropping low-variance features, Maximum likelihood estimation","Using prior expertise, Reading study documentation, Conducting a literature review, Using constructed variables",Linear regression/OLS,Inspecting regression coefficients,
Sargsyan,Anahit Sargsyan,"New York University, Abu Dhabi, UAE","\item Anahit Sargsyan, New York University, Abu Dhabi, UAE",Anahit_Sargsyan,"Model-based imputation, Adding missingness indicators, Dropping low-variance features, kNN (k-Nearest Neighbors) imputation algorithm","Model-based feature selection (e.g. F-test or LASSO), Extra Trees Regressor algorithm, Randomized Lasso","Linear regression/OLS, Logistic regression/logit, Decision tree, Random forest","MSE performance in training set, MSE performance on leaderboard, Assessing groups/clusters of features",
Schaffner,Thomas Schaffner,"Independent* (work was done while at the ""Department of Computer Science, Princeton University, Princeton, NJ 08540, USA"")","\item Thomas Schaffner, Independent* (work was done while at the ""Department of Computer Science, Princeton University, Princeton, NJ 08540, USA"")",t.f.schaffner,"Mean/median/mode imputation, Dropping low-variance features","Mutual information, Model-based feature selection (e.g. F-test or LASSO)","Linear regression/OLS, Decision tree, Random forest, LASSO, Data-based hyperparameter selection (e.g. cross validation), Support Vector Regression, Bernoulli Naive Bayes, Multinomial Naive Bayes,  K-Nearest Neighbors","MSE performance in training set, MSE performance on leaderboard, R^2, precision/accuracy/f1-score performance in cross-validation within the training set",
Schnabel,Landon Schnabel,"Stanford University, Stanford, CA, 94305, USA","\item Landon Schnabel, Stanford University, Stanford, CA, 94305, USA",lpschnab,"Mean/median/mode imputation, Imputation based on survey structure (e.g. skips), Variable standardization/rescaling, Adding missingness indicators, Inferring feature types (e.g. categorical or continuous)","Using prior expertise, Reading study documentation, Using constructed variables",Linear regression/OLS,"MSE performance in training set, Inspecting regression coefficients","This isn't about other methodological choices, but I wanted to note somewhere that I submitted individual predictions (under username lpschnab) and was also a part of a group (IU_Sociology). My descriptions refer to my individual submission."
Schonfeld,Bryan Schonfeld,"Department of Politics, Princeton University, Princeton, NJ 08544, USA.","\item Bryan Schonfeld, Department of Politics, Princeton University, Princeton, NJ 08544, USA.",signoret,Constructing your own train/test split,"Conducting a literature review, Model-based feature selection (e.g. F-test or LASSO)","Linear regression/OLS, Logistic regression/logit",MSE performance in training set,
Sender,Ben Sender,"Department of Economics, Princeton University, Princeton, NJ, 08544, USA","\item Ben Sender, Department of Economics, Princeton University, Princeton, NJ, 08544, USA",sender,"Mean/median/mode imputation, Model-based imputation, Variable standardization/rescaling, Inferring feature types (e.g. categorical or continuous), Constructing your own train/test split",Model-based feature selection (e.g. F-test or LASSO),"Linear regression/OLS, Logistic regression/logit, Random forest, Ridge regression, LASSO, Neural networks",MSE performance on leaderboard,
Stanescu,Diana M. Stanescu,"Department of Politics, Princeton University, Princeton, NJ, 08544, USA","\item Diana M. Stanescu, Department of Politics, Princeton University, Princeton, NJ, 08544, USA",haixiaow,"Mean/median/mode imputation, Model-based imputation, Variable standardization/rescaling, Dropping low-variance features","Reading study documentation, Leveraging time structure and repeated measures (e.g. lagged DVs), Mutual information, Model-based feature selection (e.g. F-test or LASSO)","Linear regression/OLS, Logistic regression/logit, Ridge regression, LASSO, Elastic net","MSE performance in training set, MSE performance on leaderboard",See Erik (Haixiao) Wang's submission
Su-Russell,Chang Su-Russell,"Department of Family and Consumer Sciences, Illinois State University, Normal, IL 61790, USA","\item Chang Su-Russell, Department of Family and Consumer Sciences, Illinois State University, Normal, IL 61790, USA",jeremykanter,"Dropping low-variance features, Maximum likelihood estimation","Using prior expertise, Reading study documentation, Conducting a literature review, Using constructed variables",Linear regression/OLS,Inspecting regression coefficients,
Tang,Jonathan D. Tang,"Department of Computer Science, Princeton University, Princeton, NJ 08544, USA","\item Jonathan D. Tang, Department of Computer Science, Princeton University, Princeton, NJ 08544, USA",cjqian,"Imputation based on survey structure (e.g. skips), Adding missingness indicators, Dropping low-variance features",Model-based feature selection (e.g. F-test or LASSO),"Linear regression/OLS, Ridge regression, LASSO, Elastic net, Lasso + Least Angle Regression","MSE performance in training set, MSE performance on leaderboard, Inspecting regression coefficients, Interpreting hyperparameters",Imputing the labeled data and using the output as additional training data; using k-folds to evaluate the performance of different configurations
Tsurkov,Emma Tsurkov,Stanford University,"\item Emma Tsurkov, Stanford University",ETsurkov,Adding missingness indicators,Using prior expertise,Logistic regression/logit,"Inspecting regression coefficients, Measures of variable importance (e.g. marginal effects)",
Turunç,Gamze Turunç,"Department of Psychology, Koc University, _stanbul, Turkey ","\item Gamze Turunç, Department of Psychology, Koc University, _stanbul, Turkey ",gturunc16,"Model-based imputation, Variable standardization/rescaling, Inferring feature types (e.g. categorical or continuous), Dropping low-variance features","Using prior expertise, Reading study documentation, Using constructed variables","Linear regression/OLS, Logistic regression/logit",Inspecting regression coefficients,
TÙzÙn,GÙlnihal TÙzÙn,"Department of Economics, Koç University, Sariyer, _stanbul/TURKEY","\item GÙlnihal TÙzÙn, Department of Economics, Koç University, Sariyer, _stanbul/TURKEY",gulnihaltuzun,"Mean/median/mode imputation, Variable standardization/rescaling, Inferring feature types (e.g. categorical or continuous)","Using prior expertise, Reading study documentation, Conducting a literature review","Linear regression/OLS, Logistic regression/logit","MSE performance in training set, MSE performance on leaderboard, Inspecting regression coefficients, Interpreting hyperparameters",-
van Loon,Austin van Loon,"Department of Sociology, Stanford University, Stanford, CA 94305, USA ","\item Austin van Loon, Department of Sociology, Stanford University, Stanford, CA 94305, USA ",Alpaca_CultureAsAWoolkit,"Variable standardization/rescaling, One-hot encoding, Inferring feature types (e.g. categorical or continuous)","Using prior expertise, Using constructed variables","Linear regression/OLS, Logistic regression/logit","Inspecting regression coefficients, R-squared and psuedo R-squared",
Varol,Onur Varol,"Center for Complex Network Research in Northeastern University Networks Science Institute, Boston MA USA","\item Onur Varol, Center for Complex Network Research in Northeastern University Networks Science Institute, Boston MA USA",ovarol,"Model-based imputation, Imputation based on survey structure (e.g. skips), Adding missingness indicators, Constructing your own train/test split","Reading study documentation, Conducting a literature review, Leveraging time structure and repeated measures (e.g. lagged DVs)","Random forest, Data-based hyperparameter selection (e.g. cross validation), Ensembling","MSE performance in training set, MSE performance on leaderboard, Measures of variable importance (e.g. marginal effects)",
Ver Steeg,Greg Ver Steeg,"Information Sciences Institute, University of Southern California, Marina del Rey, CA 90292","\item Greg Ver Steeg, Information Sciences Institute, University of Southern California, Marina del Rey, CA 90292",ryanjgallag,"Mean/median/mode imputation, Variable standardization/rescaling, Inferring feature types (e.g. categorical or continuous), Dropping low-variance features, Constructing your own train/test split","Reading study documentation, Mutual information, Model-based feature selection (e.g. F-test or LASSO)","Logistic regression/logit, LASSO, CorEx (Ver Steeg & Galstyan, 2014)","MSE performance in training set, Assessing groups/clusters of features",
Wang,Xiafei Wang,"School of Social Work, David B. Falk College of Sport and Human Dynamics, Syracuse University, NY, 13244","\item Xiafei Wang, School of Social Work, David B. Falk College of Sport and Human Dynamics, Syracuse University, NY, 13244",aprilfeifei,Mean/median/mode imputation,Model-based feature selection (e.g. F-test or LASSO),LASSO,"MSE performance in training set, Inspecting regression coefficients, Measures of variable importance (e.g. marginal effects)",
Wang,Zhi Wang,"School of Public Health/ School of Informatics, Computing & Engineering, Indiana University, Bloomington, IN 47408, USA","\item Zhi Wang, School of Public Health/ School of Informatics, Computing & Engineering, Indiana University, Bloomington, IN 47408, USA",IU_Sociology,"Mean/median/mode imputation, Variable standardization/rescaling, One-hot encoding, Adding missingness indicators, Inferring feature types (e.g. categorical or continuous), Constructing your own train/test split","Using prior expertise, Reading study documentation, Conducting a literature review, Using constructed variables, Model-based feature selection (e.g. F-test or LASSO), Excluded 'constructed scales' with more than 30% missing values","Random forest, Data-based hyperparameter selection (e.g. cross validation)","MSE performance in training set, MSE performance on leaderboard, Inspecting regression coefficients, Interpreting hyperparameters",
Wang,Julia Wang,Princeton '17,"\item Julia Wang, Princeton '17",kouyang,"Mean/median/mode imputation, One-hot encoding, Inferring feature types (e.g. categorical or continuous), Using multiple analytic datasets to generate predictions, Constructing your own train/test split","Using prior expertise, Reading study documentation, Mutual information, Model-based feature selection (e.g. F-test or LASSO)","Linear regression/OLS, Ridge regression, LASSO, Elastic net, Data-based hyperparameter selection (e.g. cross validation), Kernel methods (e.g. kernel ridge regression or support vector machines)","MSE performance in training set, MSE performance on leaderboard, Inspecting regression coefficients, Assessing groups/clusters of features",
Wang,Flora Wang,"Princeton University, Princeton, NJ 08544, USA","\item Flora Wang, Princeton University, Princeton, NJ 08544, USA",fw,"Mean/median/mode imputation, Inferring feature types (e.g. categorical or continuous), Dropping low-variance features","Reading study documentation, Model-based feature selection (e.g. F-test or LASSO)","Logistic regression/logit, Ridge regression, LASSO, Elastic net, Data-based hyperparameter selection (e.g. cross validation)","MSE performance in training set, MSE performance on leaderboard, Inspecting regression coefficients",
Weissman,Samantha Weissman,"Department of Computer Science, Princeton University, Princeton, NJ 08544, USA","\item Samantha Weissman, Department of Computer Science, Princeton University, Princeton, NJ 08544, USA",samantha_malte,"Mean/median/mode imputation, Imputation based on survey structure (e.g. skips), Variable standardization/rescaling, Dropping low-variance features",Model-based feature selection (e.g. F-test or LASSO),"Linear regression/OLS, Decision tree, Random forest, Kernel methods (e.g. kernel ridge regression or support vector machines)","MSE performance in training set, MSE performance on leaderboard, Interpreting hyperparameters",
Whitaker,Kirstie Whitaker,"The Alan Turing Institute, London, UK; Department of Psychiatry, University of Cambridge, Cambridge, UK","\item Kirstie Whitaker, The Alan Turing Institute, London, UK; Department of Psychiatry, University of Cambridge, Cambridge, UK",bz247,Information provided by team member Bingyu Zhao,Information provided by team member Bingyu Zhao,Information provided by team member Bingyu Zhao,Information provided by team member Bingyu Zhao,Information provided by team member Bingyu Zhao
Wolters,Maria K Wolters,"School of Informatics, University of Edinburgh, Edinburgh, UK","\item Maria K Wolters, School of Informatics, University of Edinburgh, Edinburgh, UK",bz247,"Creating synthetic features with PCA, please use answers submitted by Bingyu Zhao","Using prior expertise, Reading study documentation, please use answers submitted by Bingyu Zhao",Please use answers submitted by Bingyu Zhao,please use answers submitted by Bingyu Zhao,
Woon,Wei Lee Woon,"Expedia Inc., 333 108th AVE NE, Bellevue, WA, 98004, USA","\item Wei Lee Woon, Expedia Inc., 333 108th AVE NE, Bellevue, WA, 98004, USA",Anahit_Sargsyan,"Model-based imputation, Adding missingness indicators, Dropping low-variance features, kNN (k-Nearest Neighbors) imputation algorithm","Model-based feature selection (e.g. F-test or LASSO), Extra Trees Regressor algorithm, Randomized Lasso","Linear regression/OLS, Logistic regression/logit, Decision tree, Random forest","MSE performance in training set, MSE performance on leaderboard, Assessing groups/clusters of features",
Wu,James Wu,"New York University, 246 Greene Street, 3rd Floor, New York, NY 10003","\item James Wu, New York University, 246 Greene Street, 3rd Floor, New York, NY 10003",carnegien,"Mean/median/mode imputation, Imputation based on survey structure (e.g. skips), Dropping low-variance features",Model-based feature selection (e.g. F-test or LASSO),"Bayesian additive regression trees (BART), LASSO, Data-based hyperparameter selection (e.g. cross validation)",MSE performance on leaderboard,
Wu,Catherine Wu,"Department of Computer Science, Princeton University, Princeton, NJ 08544, USA.","\item Catherine Wu, Department of Computer Science, Princeton University, Princeton, NJ 08544, USA.",bleizman,"Mean/median/mode imputation, Variable standardization/rescaling, Constructing your own train/test split","Conducting a literature review, Mutual information, Model-based feature selection (e.g. F-test or LASSO)","Ridge regression, LASSO, Elastic net, Neural networks, K-Nearest Neighbors","MSE performance in training set, MSE performance on leaderboard, Inspecting regression coefficients",
Yang,kengran Yang,"Department of Civil and Environmental Engineering, Princeton University, Princeton, NJ 08544, USA","\item kengran Yang, Department of Civil and Environmental Engineering, Princeton University, Princeton, NJ 08544, USA",hty,Mean/median/mode imputation,Model-based feature selection (e.g. F-test or LASSO),"Random forest, Elastic net",MSE performance on leaderboard,
Yin,Jingwen Yin,Swiss Finance Institute & University of Lugano,"\item Jingwen Yin, Swiss Finance Institute & University of Lugano",aurora1994,Mean/median/mode imputation,Boruta: Wrapper Algorithm for All Relevant Feature Selection,"Random forest, Stochastic Gradient Boosting, Linear Discriminant Analysis",MSE performance in training set,
Zhao,Bingyu Zhao,"Department of Engineering, University of Cambridge, Cambridge, CB2 1PZ, UK","\item Bingyu Zhao, Department of Engineering, University of Cambridge, Cambridge, CB2 1PZ, UK",bz247,"Mean/median/mode imputation, Variable standardization/rescaling, Creating synthetic features with PCA, Inferring feature types (e.g. categorical or continuous), Dropping low-variance features",Using constructed variables,"Linear regression/OLS, Logistic regression/logit",MSE performance on leaderboard,
Zhu,Chenyun Zhu,"Department of Statistics, Columbia University, New York, NY 10027, USA","\item Chenyun Zhu, Department of Statistics, Columbia University, New York, NY 10027, USA",aurora1994,"Mean/median/mode imputation, Adding missingness indicators, Constructing your own train/test split","Reading study documentation, Using constructed variables, Model-based feature selection (e.g. F-test or LASSO), Used Boruta package to select features","Linear regression/OLS, Random forest, Data-based hyperparameter selection (e.g. cross validation), Kernel methods (e.g. kernel ridge regression or support vector machines)","MSE performance in training set, MSE performance on leaderboard",
x,Alex Pentland,MIT,x,Pentlandians,"Model-based imputation, Functional form transformation (e.g. logging or top-coding), Adding missingness indicators, Inferring feature types (e.g. categorical or continuous), Dropping low-variance features, Using multiple analytic datasets to generate predictions","Using prior expertise, Reading study documentation, Conducting a literature review, Using constructed variables, Leveraging time structure and repeated measures (e.g. lagged DVs), F-test, Mutual information, Model-based feature selection","Decision tree, Random forest, Gradient-boosted trees, Bayesian additive regression trees (BART), Ridge regression, LASSO, Elastic net, Data-based hyperparameter selection (e.g. cross validation), Manual hyperparameter setting, Kernel methods (e.g. kernel ridge regression or support vector machines)","MSE performance in training set, MSE performance on leaderboard",
x,Thomas Davidson,"Cornell University, Department of Sociology",x,tdavidson,"Mean/median/mode imputation, Variable standardization/rescaling, One-hot encoding, Inferring feature types (e.g. categorical or continuous), Dropping low-variance features, Constructing your own train/test split",Model-based feature selection,"Linear regression/OLS, Logistic regression/logit, Random forest, Ridge regression, LASSO, Elastic net, Data-based hyperparameter selection (e.g. cross validation), Neural networks, Kernel methods (e.g. kernel ridge regression or support vector machines)","MSE performance in training set, MSE performance on leaderboard, Measures of variable importance (e.g. marginal effects)",
x,Jennie E. Brand,"Departments of Sociology and Statistics, UCLA",x,pups33,"Model-based imputation, Variable standardization/rescaling, Adding missingness indicators","Using prior expertise, Conducting a literature review, Model-based feature selection",Logistic regression/logit,"MSE performance in training set, MSE performance on leaderboard, Inspecting regression coefficients, Assessing groups/clusters of features",
x,Abdullah Almaatouq,Massachusetts Institute of Technology,x,amaatouq,"Mean/median/mode imputation, Imputation based on survey structure (e.g. skips), Variable standardization/rescaling, One-hot encoding, Adding missingness indicators, Inferring feature types (e.g. categorical or continuous), Dropping low-variance features, Constructing your own train/test split","Conducting a literature review, Using constructed variables, Mutual information","Random forest, Data-based hyperparameter selection (e.g. cross validation)","MSE performance on leaderboard, Measures of variable importance (e.g. marginal effects)",
x,Kivan Polimis,"Center for the Study of Demography and Ecology, University of Washington",x,FormidableFamily,"Mean/median/mode imputation, Model-based imputation, Imputation based on survey structure (e.g. skips), Constructing your own train/test split","Using prior expertise, Reading study documentation, Using constructed variables","Linear regression/OLS, Logistic regression/logit","MSE performance in training set, MSE performance on leaderboard",
x,Khaled Al-Ghoneim,Hawaz,x,KAG,"Mean/median/mode imputation, Imputation based on survey structure (e.g. skips), Variable standardization/rescaling, One-hot encoding, Adding missingness indicators, Inferring feature types (e.g. categorical or continuous), Dropping low-variance features, Constructing your own train/test split","Reading study documentation, Conducting a literature review, Using constructed variables, Mutual information","Random forest, Data-based hyperparameter selection (e.g. cross validation), Ensembling","MSE performance in training set, Measures of variable importance (e.g. marginal effects), Weights based on out of bag performance of each random forest",
x,Antje Kirchner,RTI International,x,FormidableFamily,,,,,
x,Louis Raes,Tilburg University,x,LouisR,"Mean/median/mode imputation, Variable standardization/rescaling, One-hot encoding, Creating synthetic features with PCA, Dropping low-variance features, Using multiple analytic datasets to generate predictions, Constructing your own train/test split","Reading study documentation, Conducting a literature review, Using constructed variables","Gradient-boosted trees, Elastic net, Data-based hyperparameter selection (e.g. cross validation), Cubist, Mars","MSE performance in training set, MSE performance on leaderboard, Inspecting regression coefficients, Measures of variable importance (e.g. marginal effects)",
x,Daniel E Rigobon,"Department of Operations Research and Financial Engineering, Princeton University",x,drigobon,"Mean/median/mode imputation, One-hot encoding, Creating synthetic features with PCA, Adding missingness indicators, Dropping low-variance features","Using constructed variables, Mutual information, Sparse Linear Models","Linear regression/OLS, Random forest, LASSO, Kernel methods (e.g. kernel ridge regression or support vector machines)","MSE performance in training set, MSE performance on leaderboard",
x,"Alex ""Sandy"" Pentland",Massachusetts Institute of Technology,x,Pentlandians,,,Ensembling,MSE performance on leaderboard,
x,Drew M Altschul,"Mental Health Data Science Scotland, Department of Psychology, The University of Edinburgh, Edinburgh, UK, EH8 9JZ",x,dremalt,"Model-based imputation, Variable standardization/rescaling, Dropping low-variance features","Using prior expertise, Reading study documentation, Model-based feature selection","Linear regression/OLS, Logistic regression/logit, Decision tree, Random forest, Gradient-boosted trees, Elastic net","MSE performance in training set, MSE performance on leaderboard","No, my approach was relatively simple and straight-forward."
x,Caitlin E. Ahearn,University of California _ Los Angeles,x,pups33,"Model-based imputation, Imputation based on survey structure (e.g. skips), Variable standardization/rescaling, Adding missingness indicators","Using prior expertise, Reading study documentation, Conducting a literature review, Using constructed variables, Leveraging time structure and repeated measures (e.g. lagged DVs)","Linear regression/OLS, Logistic regression/logit","MSE performance on leaderboard, Inspecting regression coefficients, Measures of variable importance (e.g. marginal effects)",
x,Nicole Bohme Carnegie,Montana State University,x,carnegien,"Mean/median/mode imputation, Imputation based on survey structure (e.g. skips), Adding missingness indicators, Inferring feature types (e.g. categorical or continuous), Dropping low-variance features","Reading study documentation, Using constructed variables, Model-based feature selection","Bayesian additive regression trees (BART), Data-based hyperparameter selection (e.g. cross validation), Manual hyperparameter setting","MSE performance in training set, MSE performance on leaderboard",
x,Yoshihiko Suhara,Massachusetts Institute of Technology,x,sy,"Mean/median/mode imputation, One-hot encoding, Adding missingness indicators","Using prior expertise, Reading study documentation, Using constructed variables","Gradient-boosted trees, Ridge regression, Data-based hyperparameter selection (e.g. cross validation), Manual hyperparameter setting, Ensembling",MSE performance in training set,
x,Ryan James Compton,"University of California, Santa Cruz",x,rcompton,"Mean/median/mode imputation, Creating synthetic features with PCA, Dropping low-variance features, Constructing your own train/test split",Model-based feature selection,"Decision tree, Random forest, Gradient-boosted trees, Data-based hyperparameter selection (e.g. cross validation), Ensembling","MSE performance in training set, MSE performance on leaderboard",Oversampling to handle skewed distributions
x,Stephen McKay,University of Lincoln,x,the_Brit,"Imputation based on survey structure (e.g. skips), One-hot encoding, Functional form transformation (e.g. logging or top-coding), Inferring feature types (e.g. categorical or continuous), Dropping low-variance features","Using prior expertise, Reading study documentation, Using constructed variables, Leveraging time structure and repeated measures (e.g. lagged DVs)","Linear regression/OLS, Logistic regression/logit, Random forest","MSE performance in training set, MSE performance on leaderboard, Inspecting regression coefficients",
x,Allison C Morgan,"Department of Computer Science, University of Colorado, Boulder, CO 80309",x,amorgan,"Variable standardization/rescaling, Inferring feature types (e.g. categorical or continuous)",Using constructed variables,"Linear regression/OLS, Random forest, LASSO",MSE performance on leaderboard,
x,Ridhi Kashyap,University of Oxford,x,FormidableFamily,,,,,
x,Connor Gilroy,University of Washington,x,FormidableFamily,"Mean/median/mode imputation, Model-based imputation, Inferring feature types (e.g. categorical or continuous), Dropping low-variance features, Using multiple analytic datasets to generate predictions","Using prior expertise, Reading study documentation, Conducting a literature review, Using constructed variables","Linear regression/OLS, Logistic regression/logit, Elastic net, Data-based hyperparameter selection (e.g. cross validation), Manual hyperparameter setting","MSE performance in training set, MSE performance on leaderboard",
x,Connor Gilroy,University of Washington,x,FormidableFamily,"Model-based imputation, Inferring feature types (e.g. categorical or continuous), Dropping low-variance features",,Ridge regression,"MSE performance in training set, MSE performance on leaderboard",
x,Adaner Usmani,Harvard University,x,FormidableFamily,,,,,
x,Claudia V. Roberts,Princeton University,x,chicacvr,"Mean/median/mode imputation, Variable standardization/rescaling, Constructing your own train/test split","Using prior expertise, Reading study documentation, Conducting a literature review, F-test, Mutual information, Model-based feature selection","Linear regression/OLS, Decision tree, Ridge regression, LASSO, Elastic net, Data-based hyperparameter selection (e.g. cross validation)","MSE performance in training set, MSE performance on leaderboard, Inspecting regression coefficients",
x,Eaman Jahani,MIT,x,eaman,"Mean/median/mode imputation, Variable standardization/rescaling, One-hot encoding, Functional form transformation (e.g. logging or top-coding), Adding missingness indicators, Inferring feature types (e.g. categorical or continuous), Dropping low-variance features","Using prior expertise, Reading study documentation, Using constructed variables, Mutual information, Model-based feature selection","Random forest, LASSO, Elastic net, Data-based hyperparameter selection (e.g. cross validation), Ensembling","MSE performance in training set, MSE performance on leaderboard",multi task elastic net to use the correlation structure between DVs
x,Brian J. Goode,Virginia Tech,x,bjgoode,"Mean/median/mode imputation, Imputation based on survey structure (e.g. skips), Variable standardization/rescaling, One-hot encoding, Inferring feature types (e.g. categorical or continuous), Constructing your own train/test split","Using prior expertise, Reading study documentation, Using constructed variables","Linear regression/OLS, Logistic regression/logit, LASSO, Data-based hyperparameter selection (e.g. cross validation)",MSE performance in training set,
x,Debanjan Datta,Virginia Tech,x,bjgoode,"Model-based imputation, Imputation based on survey structure (e.g. skips), Constructing your own train/test split","Reading study documentation, Conducting a literature review, Using constructed variables, Model-based feature selection","LASSO, Data-based hyperparameter selection (e.g. cross validation)","MSE performance in training set, MSE performance on leaderboard",#NAME?
x,Anna Filippova,GitHub,x,FormidableFamily,"Inferring feature types (e.g. categorical or continuous), Logical imputation of caregiver properties based on other information in the survey",Using constructed variables,LASSO,"MSE performance in training set, MSE performance on leaderboard",
x,Erik H. Wang,"PhD Candidate, Department of Politics, Princeton University",x,haixiaow,"Mean/median/mode imputation, Model-based imputation, Imputation based on survey structure (e.g. skips), Dropping low-variance features",Model-based feature selection,LASSO,Inspecting regression coefficients,"We use LASSO as a preprocessing method in the following sense: After some very early stage of cleaning including dropping variables (documented in the paragraph below), we mean impute the remaining variables and perform LASSO. After this, we drop variables that have coefficients zero."
Mahajan,Mayank Mahajan,"Department of Computer Science, Princeton University, Princeton, NJ 08544, USA","\item Mayank Mahajan, Department of Computer Science, Princeton University, Princeton, NJ 08544, USA",kapoor,"Mean/median/mode imputation, Model-based imputation, Variable standardization/rescaling, Dropping low-variance features, Constructing your own train/test split","Using prior expertise, Conducting a literature review, Model-based feature selection (e.g. F-test or LASSO)","Linear regression/OLS, Logistic regression/logit, Gradient-boosted trees, Ridge regression, LASSO, Elastic net, Data-based hyperparameter selection (e.g. cross validation)","MSE performance in training set, MSE performance on leaderboard, Measures of variable importance (e.g. marginal effects)",
Argyle,Lisa P. Argyle,"Department of Political Science, Brigham Young University","\item Lisa P. Argyle, Department of Political Science, Brigham Young University",largyle,"Mean/median/mode imputation, Adding missingness indicators","Using prior expertise, Reading study documentation, Leveraging time structure and repeated measures (e.g. lagged DVs)",Linear regression/OLS,Inspecting regression coefficients,