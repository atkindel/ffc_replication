names,account,narrative
"Muna Adem, Andrew Halpern-Manners, Patrick C. Kaminski, Helge-Johannes Marahrens, Landon Schnabel, and Zhi Wang",IU_Sociology,"Our approach rests on a combination of social science theory and machinelearning methods. We first developed a theoretically-informed list of variables weexpected to be important. We then augmented this list with highly predictivevariables selected by a LASSO regression. All variables in the augmented listwere verified using domain knowledge. Finally, using the complete list, wetrained a random forest regressor / classifier, and tuned its hyperparameterswith cross-validation."
Abdulla Alhajri,alhajri,Model performance for the leaderboard and holdout sets was determined by looking at the improvement over the baseline - or relative accuracy improvement.
"Bedoor AlShebli, Areg Karapetyan, Anahit Sargsyan, and Wei Lee Woon",Anahit_Sargsyan,"The employed approach resorts to machine learning techniques for devising a predictive model for GPA with a particular focus on explicability of the results produced when considering the nuanced variations between subjects. To facilitate the analysis of the data, a number of pre-processing steps were carried out: (i) all missing and negative values were replaced by NaN and the columns with 0 variance were removed, (ii) , columns with more than 400 NaN values were dropped, (iii) the variant of kNN (k-Nearest Neighbors) imputation algorithm was leveraged to estimate the NaN values.  Next, a manifold of filter- and wrapper-based methods, including Principal Component Analysis,  Ridge regression, Lasso,  and Gradient Boosting  Regression,  were  attempted in  search  of  the  most  informative  feature  subset  of  reasonable cardinality. These methods were applied to the extracted pool of features,  both recursively and explicitly,  and probed under diverse parameter settings. The acquired subsets were then evaluated for their predictive accuracy across various models trained. The target subset of optimally descriptive features, as revealed by extensive experiments, was obtained by the following means. Feature importances were estimated by the Extra Trees Regressor algorithm and Randomized Lasso, and the top 500 features were retained from each.  For the latter, two different values were considered for the regularization parameter, thus resulting in two separate feature subsets. The  intersection  of  these  three  subsets,  containing 69 features,  led  to  maximized  GPA  prediction  accuracy. More concretely,  with the Random Forest algorithm,  a mean squared error of approximately 0.363 was achieved, allowing these results to be placed in the top quartile of the final FFC scoreboard. "
Redwane Amin,spike_slab_team,"Before using machine learning techniques to predict the outcomes, preparing the data and selecting features are crucial steps. In addition to statistical techniques, investing time in analyzing the study documentation allowed to filter out variables which would have otherwise added noise or led to over-fitting. Then for each chosen statistical learning method, we tuned hyper-parameters (where applicable) on the training set using cross validation and evaluated their performance on the held-out validation set."
Ryan B Amos and Guanhua He,rbamos,"Our most successful insight was the use of feature selection. We tried a variety of feature selection techniques, and found k-means to be the most effective technique. We found using 50 clusters provided the best results, which means most of the data can be well represented by just 50 variables. The most effective machine learning techniques on the clustered data were elastic net regularization for continuous outcomes and a support vector machine trained with stochastic gradient descent for discrete outcomes. We tried a variety of imputation techniques, but ultimately we found that the naive method of imputing the data to the mode was just as effective as more targeted imputation methods."
Livia Baer-Bositis,lbb285,The models to predict each of the six outcome variables were built around the concept that the past predicts the future. The key explanatory variables were all constructed from data collected in year 9 of the study including a scaled measure of hardship and selected via trial and error.
Moritz Büchi,mdb,"The first step was to obtain a complete data set using multiple imputation by chained equations. The variable selected as the outcome was material hardship. The approach in this submission was to show that a simple linear model may produce smaller mean squared errors than benchmark models even when the selected predictors are theoretically uninterpretable, pointing to the often opposing analytical goals of prediction versus explanation."
Bo-Ryehn Chung and Flora Wang,fw,"Our methods and models were parsimonious in complexity, but still managed to perform higher than average in at least one outcome. We first removed variables with low variance (mainly those of string types) to reduce the dataset dimensionality. We then performed median and mode-based imputation on variables with missing or certain no response codes, as most outcomes were of skewed distributions. We then evaluated various regularized regression methods that selected important features, and applied median importance weights to these features with cross-validation for the final model. The elastic net model with cross validation performed the best based on metrics as MSE and confidence intervals of the cross validation scores. We made sure to assess the features selected and their coefficients using literature reviews and the study documentation. Other regularized regression methods either overfit the data (ridge regression) or did not select enough variables that made intuitive sense (due to the random nature of LASSO feature selection for correlated variables). We found that the elastic net model had a good balance of finding correlated variables abundant in our longitudinal dataset and selecting enough features for the final model."
William Eggert,weggert,"It was interesting to see that classifying Eviction on the validation set yielded an accuracy of greater than 90% out of the box; in fact PCA and K-Best feature selection often made the accuracy worse. Additionally,  this challenge was very susceptible to choosing hyperparameters that produced excellent accuracy, but trivial results (e.g. predicting GPA but only producing middle-of-the-road GPAs). Therefore, careful feature space reduction is of utmost importance.  Voting classifiers were the most robust against this pitfall.  However, a Gaussian Mixture Model Process served to be the most promising avenue for worthwhile results.  A combination of domain-expert collaboration to reduce the feature space, with a GMM, is predicted to produce the best results."
Gregory Faletto,greg.faletto,"I only trained models for the continuous responses. I relied on the constructed variables. I trained a lasso model and a principal components model on each continuous response. For the lasso model, I chose the tuning/penalty parameter by cross-validation. For the principal components regression, I chose the number of principal components to include by cross-validation. Finally, I chose which of these models to use by comparing the mean squared error of each model."
Zhilin Fan,ADSgrp5,"Given all the background data from birth to year 9 and some training data from year 15,we infer six key outcomes (gpa, grit, material_hardship, eviction, layoff, jobtraining) in the year 15 test data. In the data cleaning process, we deal with categorical variable and continuous variables separately, for continuous variable, we replace the NA with the median value of that variable, and create a new categorical variable to indicate the NAs (where the NAs may contain information to some degree) ,attach the new indicating categorical variable to the oringinal categorical features.For categorical features, we replace the NAs with a number that doesn't exist in original data set and transform every categorical to a dummy matrix, for every dummy matrix whose elements are either 0 or 1, we choose the 2nd to last column to avoid col-linearity. Given the cleaned data, our team work on different directions, one team work on different features and one team work on different machine learning tools, since we got to know the xgboost apparently outperforms other methods, we together work on features selected from various angles. For case 1: data obtained when children are at age 9 and only consider the continuous variables.Case 2: data obtained when children are at age 9, use categorical variables. Then we bag them by using the weighted average. We use the same strategy to other continuous outcomes( grit, material hardship)."
Jeremy Freese,jeremyfreese,"I did two things.  More seriously but less time-consumingly, I just fit models that seemed to make some intuitive sense to me as models.  I did not do anything fancy here, nor did I have any illusions that these would rise to the top, but I was using the challenge for pedagogical purposes.  Also, as a lark, I played around with generating a bunch of prediction sets with small differences and seeing if I could infer the values of outcomes in the quiz set.  This worked for the rare outcome.  I thought this might provide some great advantage--basically it would allow me to generate predictions using both the testing + quiz set--but after the challenge I was informed by MS that there was something that they had done (I forget exactly what) that thwarted this strategy."
Josh Gagné,jgagne,GLM with mean imputation and predictions shrunk toward the training set mean.
Sonia P. Hashim and Viola Mocz,shashim,"We worked as a team to predict gpa, grit, and materialHardship. We experimented with multiple models, using ordinary linear regression, lasso regression and ridge regression after conducting imputation, feature engineering, and feature selection on the expanded feature set. To impute missing values, we tested median single imputation, K-Nearest Neighbors single imputation, and multiple imputation using the Amelia package in R. Also, if there were columns with one value in addition to NA we converted these into binary variables where 0 indicated missing data and 1 indicated present data. We engineered features by taking the mean of matching inputs and by using maximum pooling in order to combine features using similar questions asked across years. We also conducted feature selection by removing features with a low frequency of observations and low variance, features that were highly correlated with each other, and features with low random forest importance. Five-fold cross-validation was used to evaluate the efficacy of our models on the available training data. Our final submission used ordinary linear regression with median imputation, missing data indicators, engineered features, and feature selection using variance. "
Sonia A. Hausen,shausen,"I cleaned and recoded the data, experimenting with all 6 outcomes using OLS and logit. Coming from a sociology background and studying well-being, I looked for an all-encompassing variable, like overall life satisfaction, which would capture many of the other variables within it (like abuse, income, job status, mental health etc.). I hypothesized that the variable ïmotherÍs overall life satisfaction at year 9, self-reportedÍ might be a good predictor given the strong influence mothers have on child outcomes. I used MSE performance on the leaderboard as a guide; my best performing models included the variable mom_sat. "
Kimberly Higuera,khiguera,"Initially, I started by focusing on how long seeded childhood characteristics could predict long term outcomes by using low birth weight as the main dependent variable. I had attended a talk on low birthweight and it's links to test scores when I was undergraduate and I wanted to investigate whether the link was robust with the fragile family data. When the link based on the coefficients and t scores did not seem statistically or socially significant across different model types and different covariates and outcomes, I decided to shift into looking at factors that were arguably even more deeply seeded than birthweight: mother's characteristics. I considered these more deeply seeded because they existed before the birth and also because they had long term interaction and thus potential long term effects on the respondents. Plus mothers seem much more impactful in that they are seem less likely to be missing from the raising of a child than a father. Following that I ended up looking at how mother's characteristics affected the likelihood of getting laid off. "
Ilana M. Horwitz,ihorwitz,"I looked at all 6 outcomes. Based on my prior knowledge, I chose the following explanatory variables: whether the mother received welfare, the length of time the mother looked for a job, whether the home had peeling paint, frequency of drinking alcohol, the father's influence on school, a child's sense of belonging in school, and whether the child saw his father in the last year. In some cases, I converted the variable into a binary outcome. The explanatory variable I used varied based on the outcome of interest. I then ran logits and OLS models to predict outcomes. "
Lisa M. Hummel,Bumblebee2023,I relied on knowledge from sociology and psychology about what factors impact outcomes for children and families and attempted to capture those in the models to predict the results. 
Naman Jain and Ahmed Musse,amusse,"In this work we build machine learning models to predict the 6 key outcomes in the childrenÍs development: GPA, grit, material hardship, eviction, layoffs, and job training. We first imputed the missing values from the survey by replacing the missing values with the mode for that category. Then, to predict the 3 binary outcomes, we performed chi-squared feature selection to get the 1000 best features. We use various binary classifiers such as Logistic Regression, K-nearest neighbors, Random Forest and other ensemble methods to predict eviction, layoffs and job training. For these outcomes, we found a tuned Random Forest Classifier to perform best given its ensemble nature and enhanced ability to restrict over-fitting. To predict the 3 continuous outcomes, we first did chi-squared feature selection to get the 1000 best features. To predict GPA, grit and material hardship, we conduct Support Vector, Lasso, Ridge and Gaussian Process regressions. Here, the simplicity (and run-time efficiency) of Lasso regression and its in-built ability to conduct model selection made it the preferred method. Our results corroborate past research showing that children in stable two parent households fare better but establish correlations with more novel features as well. We see that housing conditions in early years especially in years 1, 3 and 5 after birth are particularly predictive.for all 6 outcomes. Therefore, an appropriate policy response might be to focus efforts to promote better access to housing for families with young children."
Kun Jin and Xiafei Wang,aprilfeifei,"We worked as a group. During the data pre-processing, we deleted variables with 70% missing values and imputed the missing values of the rest variables with the mean value. For our best model, we conducted lasso regression with L1 regularization upon 4574 variables for all six outcomes to select features. To be specific, we first obtained the coefficients of the regression model using 5-fold cross-validation and the elastic method with Alpha = 0.5; further determined the largest regularization coefficients such that the mean saqured error (MSE) is within one standard error of the minimum MSE; finally, corresponding features with larger coefficients are selected and used to train the regression model. Feature normalization by scaling was followed by feature selection. Prior to our final model, we also fit linear regression, SVM and Linear Discriminant Analysis model, but neither of them yielded better results than lasso regression. "
David Jurgens,davidj,"My approach used a Random Forest (RF) classifier for categorical attributes and RF regressor for numeric attributes.  All categorical features were one-hot encoded.  During the initial design phase, I tested using PCA and SVD transformations of the features, which worsened performance and were left out of the final model.  Similarly, I also examined using mean-value imputation of missing data, which also led to worse performance with the RF models.  Both design choices were tested using 5-fold cross-validation within the training data. Since the categorical tasks had imbalanced numbers of instances for each label, I used SMOTE to oversample rare classes using synthetic instances until all classes had an equal number of instances.  This oversampling lowered the error in all of my cross-validation tests on the training data.  Finally, I optimized the hyperparameters of the model using a sweep across multiple values; ultimately, the only two hyperparameters that substantially affected performance was the minimum number of instances per leaf in the decision tree and the total number of decision trees.  Ultimately, I chose a minimum of 10 instances per the leaf, which I suspect prevents the model from overfitting by identifying more robust predictors that apply to multiple subjects, and an unnecessarily-large number of trees (10,000) to account for the large number of possible feature combinations."
E. H. Kim,ehk02004,"Matthew Desmond, according to his talk on his book Eviction, argued that having children was positively related to being evicted, implying that the disruptiveness/chaos that comes with having children and the inherent lack of calm in the household was responsible for a family being evicted. As such, in addition to looking at how ratings of how calm the atmosphere of the house is when the child is 9, I include an index variable of the child exhibiting appropriate behavior at age 9 (as our continuous variable) and I include the binary variable for whether or not a childÍs behavioral/social problems was discussed with the teacher during the last school year when the child is 9, as both seemed likely to shed light on the disruptiveness/chaos in the household that might be affiliated with eviction. In addition, it was assumed that the father's race (thinking of race literature), the mother's age at childbirth, and how often the children had a tendency to move were affiliated with layoff - the latter variables included based off of intuitive assumptions."
Ben Leizman and Catherine Wu,bleizman,"In this dataset, we imputed values for missing data by using the featureÍs highest frequency positive value and then normalized all values. We created datasets for each of the six outcomes, using only sample where the outcome is not NA. We reduced the feature set of each dataset from 12,805 to 100 using mutual information-based feature selection. To predict continuous outcomes such as GPA, Grit, and Material Hardship, we trained LASSO, Ridge, and Elastic Net regression models. To predict categorical outcomes such as Job Layoff, Job Training, and Eviction, we trained Ridge, K-Nearest Neighbors, and Multi-layer Perceptron classification models. We used default hyperparameters and selected an optimal model using k-fold cross validation. The regression models were evaluated on mean squared error and R^2, while the classification models were evaluated on precision, recall, and F1 score."
Naijia Liu,NaijiaLiu,We developed an iterative feature selection method using Ridge regression. 
Andrew E. Mack,aemack,"We trained random forest, gradient boosting and LASSO regression models using various hyper-parameters. We also used F-statistics to screen variables, with the number of variables used counting as an additional hyper-parameter. In total, we had 9 potential models for each outcome. For each of the 6 outcomes, we selected the best model using 10-fold cross-validation."
Noah Mandell,nmandell,"For a dataset as large and raw as this one, how the data is processed and cleaned can be crucial. We immediately dropped all feature columns with zero variance, along with any column containing strings. We also dropped columns with more than 30% of the data missing. Thisæreduces the number of columns significantly, but it also reduces the ratio of missing to non-missing data, which is the goal of this step. We choose the threshold of 30% missing per column because this produces a dataset with only 20% of the data missing across all columns. We then attempted to distinguish categorical variables from continuous ones by noting that the numerical coding for the categorical variables used only integers and did not use values in the range 20-99. This resulted in 90% of the variables being labeled categorical. We then used a one-hot encoding for the categorical data, and we filtered out one-hot-encoded columns with low variance. We then used Ridge regression to simultaneously predict all six outcomes. We used a nested cross-validation procedure, with an inner cross-validation loop for fitting hyper-parameters, and an outer loop for evaluating model performance."
Malte Möser,malte,"For preprocessing, I removed highly correlated features as well as those with low variance. Then, I added indicator variables for questions that were skipped or where the respondents refused to answer. I imputed all missing values in the dataset, with mean imputation for numerical features and mode imputation for categorical features, and standardized the numerical features. For prediction, I used a generalized linear model as provided by the R package `glmnet`, with hyperparameter tuning based on repeated cross-validation using the R package `caret`."
Katariina Mueller-Gastell,katamg,"I used existing sociological theory to identify features that would plausibly be correlated with other parental characteristics. I then included these features in simple OLS and logit models, taking care not to overfit the training data by using my own train/test split. For example, I found that whether the mother had breastfed any of her children was a fairly good predictor of child outcomes."
Qiankun Niu and Kengran Yang,hty,"In this project, we first explored the data from the survey with large feature sets. We observed that there are many missing values in the data so we first cleaned the data according to missing value, constant values and perfect co-linearity. Then we explored different imputation methods. After cleaning and imputing the data, we adopted various regression and classification methods and chose random forest due to the high dimensionality and complexity of variable relationship."
William P. Nowak,wnowak,"Built ensemble models using only independent features from mother, father, and other individual contributors."
Hamidreza Omidvar,hamidrezaomidvar,"First we used MICE imputation method to fill missing data. In the next step, we calculated the correlation of all features for each outcome. After this step, we selected top correlated features (either negative or positive correlation) as the main features. Finally, we used linear (for continuities outputs) and logistic (for binary outputs) regressions to predict the outputs."
Karen Ouyang and Julia Wang,kouyang,"At the time of our submission, the Fragile Families dataset was the largest and sparsest with which we had worked, so we focused on sanitizing the dataset and engineering useful features. In addition to imputing missing data, we iteratively tested different Pearson correlation coefficient thresholds to select features. Of the six models that we trained and tuned, the elastic net regression model consistently made the most accurate predictions."
Katy M. Pinto,Katy_P,"In the submissions, my approach to the Challenge was to focus on established relationships between variables to create models for the six outcomes based on prior research. I focused mainly on constructed variables. I focused on parental background (e.g. parentÍs education, race/ethnicity), household structure (e.g. marital status, income, number of children) and childÍs individual characteristics (e.g hours spent on homework, gifted) as predictors of GPA, Grit, Material Hardship, Eviction, Job Loss, and Job Training.  The attempts included OLS for the continuous outcome variables, logistic regression for the binary outcome variables. I attempted one multiple imputation technique that did not provide better fit in my model compared to recoding missing variables to mean/median/mode and including a flag for missing in models. In the end, the final models submitted were much simpler in approach, compared to some of the early models I attempted with more predictor variables. I also compared my submissions based on the leadership board and the simpler models with fewer predictors seemed to perform better than models I submitted with more predictors. "
Ethan Porter,lennyc,I only included variables for which I had a substantive reason to believe might affect the outcome. 
Kristin E. Porter and Tejomay Gadgil,mdrc,"MDRC applied several analytic steps in our predictive analytics framework to the Fragile Families Challenge (FFC) „ those focused on data processing, creating and curating measures, and modeling methods.  The following describes the underlying premises that guided our analyses: (1) Invest deeply in measure creation „ combining both substantive knowledge and automated approaches. (2) ñMissingnessî is informative and should not be ñimputed away.î (3) Eliminate unhelpful measures (those with very little variation, those that are redundant and those that did not apply to the primary care giver). (4) Evaluate ïlearnersÍ based on out-of-sample performance, using cross-validation. In MDRCÍs predictive analytics framework, we define a ñlearnerî as some combination of (a) a set of predictors, (b) a modeling method or machine learning algorithm, and (c) any tuning parameters for the corresponding machine learning algorithm. (5) Combine results from different learners with ensemble learning."
Crystal Qian and Jonathan D. Tang,cjqian,"We preprocessed the Fragile Families data through pruning based on answer ratio (i.e., features that were missing more often were regarded as less important) and mapping all string-based features to integers to make them suitable for regression. We performed imputation by training a regressor on the labeled data, with no missing values, assigned to corresponding training classes. To address and take advantage of data sparsity (~17% of cells in the dataframe were empty), we eliminated ~25% of the ~13,000 features that had the lowest ratio of non-""NA"" responses to total responses, encoded string responses, and used mean value/regression-based imputation to further prune ""NA"" responses. Afterwards, we used it to predict the missing values within the training set, as well as predicting the values for the entirety of the test classes. We concatenated the now-filled training classes and test classes, making a final prediction array. Then, we applied regression-based prediction techniques (LASSO, etc) for both the discrete and continuous predictors, in part incentivised by Brier score leaderboard scoring. We tested various regressors, including Lasso, Lasso with Least Angle Regression, Elastic net, and Ridge regression, for effectiveness in predicting GPA, Grit, Material Hardship, Eviction, Layoff, and Job Training. We did k-folds cross validation (k=5) in order to locally evaluate our different models. Ultimately, we found that Lasso regression performed the best for us on this dataset (the questions asked in this study could be highly correlated, explaining the success of Lasso regression). Using imputation in order to generate our own value-completed training data was extremely helpful. Interestingly, the three most predictive features to our models included the questions, ""In past year, you shouted, yelled, or screamed at child."", ""Is there someone you could count on to co-sign bank loan for $5000?"", and îThere were problems with neighborhood safety."""
Tamkinat Rauf,Tamkinat,"I predicted GPA, grit, material hardship, eviction, job training, and layoff. I drew on past research for initially selecting predictors, and then used MSE as well as leaderboard positions to tweak my models. In general, I used the most parsimonious models possible. I used logit models for binary outcomes and OLS for continuous outcomes. My participation in the Fragile Families Challenge was part of an exercise for a statistical methods course. While I independently conducted and submitted my predictions, this was really a joint effort with my professor, Jeremy Freese, and 15 other colleagues who took the course. We shared code for constructing variables and frequently discussed our modeling strategies."
Thomas Schaffner and Andrew Or,t.f.schaffner,"This submission consists of work by two participants. We evaluated multiple data imputation strategies and predictive models, learning predictors for each outcome variable separately. After evaluating several indicative measurements (R-squared, MSE, precision, recall, accuracy, and F1 scores), we selected an imputation strategy that replaced missing data and removed string-valued variables in conjunction with random forest predictors. We then programmatically tuned the random forest hyperparameters to arrive at our final predictions."
Landon Schnabel,lpschnab,"I produced individual predictions and also worked with a group (the IU_Sociology team). Initially, I developed more complex models using a large number of variables driven largely by what seemed to matter in the training data. I ultimately decided, however, to use a simpler and more theoretically-driven approach with just a few variables and basic methods. In my final submission, I used just the following variables and linear regression to predict GPA: parental education at baseline, the childÍs earlier score on a Woodcock-Johnson test, and the childÍs earlier ñgrit.î"
Bryan Schonfeld,signoret,"We read through the literature to find substantively important variables. We divided the data into a test set and training set, and used a variety of regression and statistical learning tools (logistic regression, linear regression, LASSO, etc) to find the best predictors."
Ben Sender,sender,"Our study leveraged seven prediction models: Linear Regression, Lasso Regression, Ridge Regression, Logistic Regression, Random Forest, Neural Network, and Naive Bayes. We imputed missing features using the mode from other years, and selected features for each outcome using chi-squared. To tune and evaluate the models we created a test set with 10% of the families from the original training set. We evaluated binary models based on accuracy, and continuous models on mean squared error and R-squared. The results for our test set were similar to the results for our Fragile Families submission."
Emma Tsurkov,ETsurkov,"My approach to the Fragile Families Challenge was based on utilizing my background in law to try and create parsimonious but effective model, focusing on the eviction outcome. Although eviction is a mostly exogenous shock, I wanted to examine it as an outcome of an institutional process. Eviction, more than the other outcomes in the Fragile Families Challenge is a result of legal action. Accordingly, I have conducted research into eviction law. I found that smoking is prohibited in public housing and that smoking even inside oneÍs housing unit without causing a fire or any damage can serve as grounds for eviction. Additionally, I found that many states and localities have strict anti-smoking laws in multi-unit buildings, and that even if there is no law prohibiting smoking in the unit, landlords can prohibit smoking and use smoking as grounds for eviction. This led me to believe that motherÍs smoking, might be a good predictor of eviction, whether as a genuine reason or a pretext used by landlords trying to remove undesirable tenants. I tested different model specifications and chose the best performing model, with basic covariates of the motherÍs education and race."
Austin van Loon,Alpaca_CultureAsAWoolkit,I used background knowledge to select a small set of variables that seemed likely to matter for the outcomes. I would iteratively (a) predict missing values of the dependent variable using my set of variables (b) find which of the remaining independent variables had the most missing values (c) remove that variable from my set of variables and (d) repeat.
Onur Varol,ovarol,"My approach consists of feature categorization and model selection. First, I parsed all codebooks to extract information about the panel, survey respondents, and keyword-based labels. Later I selected different feature groups and evaluated their performance on cross-validated random-forest models. Both feature imputation and filtering missing values are tested, and removal of the missing values are performed for all of the analysis. Features having high importance score and models having success on the leaderboard kept for the next iteration of model construction. "
Samantha Weissman,samantha_malte,"We took a systematic approach towards identifying relevant features for predicting outcomes in the Fragile Families Challenge, using a combination of imputation, feature selection, and cross-validated model selection. To impute the data we code skipped and refused answers through new binary feature vectors, replaced categorical values with the mode and numerical values with the mean of each feature, removed low variance and highly correlated features (based on Spearman correlation), converted all categorical values into indicator variables and scaled continuous variables to have a mean of 0 and unit variance. To further reduce the dimensionality of the data we used Lasso regression and elastic net regression with cross-validation to find the best hyperparameters, as well as cross-validated recursive feature elimination (RFE) based on a support vector machine with a linear kernel and a step size of 5. We implemented 5 different classifiers and regressors (AdaBoost, Gaussian Process, Linear regression, random forest, and SVM). Finally for cross-validation and hyperparameter tuning we employed different techniques per classifier/regressor, including 10-fold cross-validation and grid search, and evaluate both a linear and a Gaussian kernel."
"Yue Gao, Jingwen Yin and Chenyun Zhu",aurora1994,"We first solved the missing data issue by making NA a special level in categorical features and imputing the missing value with median in continuous features. After data cleaning and missing data imputing, we separated the features into categorical variables and continuous variables. We used random forest based feature selection method to select a few significant features for each outcome. We used Boruta Package to conduct feature selection which works as wrapper algorithm around Random Forest. Various machine learning algorithms were evaluated based on the selected features and the best algorithm for each outcome was selected using MSE.  We tried a series of models including linear regression, full tree, pruned tree, random forest, conditional inference trees, stochastic gradient boosting, support vector machine, linear bagging, ensemble linear regression and random forest, ensemble support vector and random forest, linear discriminant analysis, C5.0, and KNN. Based on the root mean squared error metric for continuous outcome variables and accuracy metric for binary outcome variables, we predicted the final results by using random forest for GPA, eviction, job training, using stochastic gradient boosting for grit, material hardship, and doing LDA for layoff. Finally we retrained the models using full dataset and submitted to the leaderboard."
"Bingyu Zhao, Kirstie Whitaker and Maria K Wolters",bz247,"The submission of our team adopted a few simple steps to make predictions. First, basic data cleaning was carried out, which involved selecting only the continuous variables as the predictors (removing all categorical variables), imputing the empty entries by the mean and removing constant (0 variance) columns. This left 1,771 variables remaining in the dataset. In the second step, Principle Component Analysis was conducted and the top 50 principle components were kept as the model covariates. In the last step, multi-variable linear regression was used to model the continuous dependent variables and logistic regression to model the binary outcomes. In the end, our submission performed better than the benchmark data in two out of the six outcome variables, including the material hardship and GPA, while both are continuous outcomes."
Caitlin E. Ahearn and Jennie E. Brand,pups33,"Our approach to the Fragile Families Challenge was to draw on empirical results from the large literature on the determinants of job loss to generate a simple model to predict caregiver layoff. We chose to focus exclusively on layoff as the outcome because Jennie Brand, a member of our team, is an expert in that area. We used prior research on layoff to build a model predicting layoff, and adjusted the model based on model fit statistics of different iterations of predictors. Our models included subsets of sociodemographic, employment, family, psychosocial, and family background characteristics. We ultimately submitted a few model specifications to the Challenge: the first was based on prior research on job loss for the general population; the second was based on prior research on job loss among disadvantaged mothers (the primary population of the Fragile Families data); and the third was a more parsimonious model that included a few key covariates. We also tested various methods of imputation, including single and multiple imputation. The results and predictions of each of these models were all quite similar. Our best performing model, and final submission, was one of our early model specifications based on covariates for predicting layoff in the general population. We chose a simple estimations strategy, and did not expect different model specifications to produce much variation, as prior research has repeatedly shown that job loss is a relatively exogenous shock."
Thomas Davidson,tdavidson,"I initially started by experimenting with all six outcomes and a range of models but decided to focus on GPA. I used simple heuristics to deal with missingness and identify variable types. I then standardized continuous variables and one-hot transformed categorical variables. To make predictions I used feed-forward neural networks, varying the depth, breadth, and activation function used."
Abdullah Almaatouq,amaatouq,"In this submission, we ran a Random Forest regressor for continuous outcomes and Random Forest classifier for the categorical outcomes. In the case of categorical outcomes, we predict the probability of positive examples rather than the binary class. All of the models in this submission were trained on the top 600 untransformed features selected by mutual information that was performed during the pre-processing step. Due to the high potential of overfitting, we ran 300 Random Forests in a nested cross-validation fashion. This means, we used a series of train/validation/test set splits, where in the inner loop, the score is maximized by fitting a model to each training set, and then maximized in selecting hyper-parameters over the validation set. In the outer loop, generalization error is estimated by averaging test set scores over several dataset splits. We then weighted each model predictions based on this outer loop score."
"Anna Filippova, Connor Gilroy, Ridhi Kashyap, Antje Kirchner, Allison C. Morgan, Kivan Polimis, and Adaner Usmani",FormidableFamily,"Recent applications in computer science have sought to incorporate human knowledge into machine learning methods to address overfitting during prediction tasks, where data sets are incomplete, and have a high ratio of variables to observations. To address these issues, we implement a ``human-in-the-loop'' approach in the Fragile Families Challenge. First, we try several different approaches for imputing missing responses: mean imputation, regression based approaches, and multiple imputation. Next, we use surveys to elicit knowledge from experts and laypeople about the importance of different variables to different outcomes. This strategy gives us the option to subset the data before prediction or to incorporate human knowledge as scores in prediction models, or both together. We incorporate this variable information and imputed data into regularized regression models. What we find is that human intervention is not obviously helpful. Human-informed subsetting reduces predictive performance, and considered alone, approaches incorporating scores perform marginally worse than approaches which do not. However, incorporating human knowledge may still improve predictive performance, and future research should consider new ways of doing so."
Khaled Al-Ghoneim ,KAG,"Using the same feature selection as the team submission, we ran multiple random forests. We combined the results using weigthed the results based on the out of bag performance score."
Louis Raes,LouisR,"My approach in making predictions was based on a cursory reading of literature on the Fragile Families and Child Wellbeing study, combined with a lot of trial and error."
Daniel E. Rigobon,drigobon,"Pre-processing of the data was done by removing features with low-variance, performing one-hot-encoding on all categorical features, and mean-imputing all missing continuous and ordinal features. Due to the large amount of covariates produced by this process, a sparse linear regression was run for each outcome to identify important features. A regularization parameter was selected to ensure that the regression’s $R^{2}$ value was close to an ad-hoc value of 0.4. Following feature selection, various learning algorithms were evaluated on splits of the training data: Principal Components Regression, Kernel Support Vector Machine, and Random Forest. The Random Forest algorithm consistently had the best performance for all outcomes. Its’ hyperparameters were selected by cross-validation, and final predictions were made with the full training set."
Alex ``Sandy'' Pentland,Pentlandians,"The Pentlandians submission is an ensemble prediction, where it aggregated four individual sets of predictions (i.e., one from elastic net, two by random forest, and another from a GBoost tree). In particular, the ensemble prediction consists of a weighted team average, in which the weights were determined by relative ranking on the leaderboard (i.e., the weight vector for the top three performing predictions for each outcome was given by (1/2, 1/3, 1/6) for first, second, and third, respectively). Predictions performing worse than 30th on the leaderboard were not included in this averaging."
Drew M. Altschul,dremalt,"I first removed many variables with low-variance, so that I could then select particular features of interest with generalized boosting. Once I had a separate set of features for each outcome variable, I made a single data subset and multiply imputed missing values. Prior to imputation I added some variables to the dataset either because they were of special theoretical interest, or because I wanted to use them to add power to the imputations. With the imputed datasets I fitted elastic net and more standard linear regression models to the dataset, and using these models I generated the predictions that I submitted to the challenge."
Nicole Bohme Carnegie and James Wu,carnegien,"The first step in our process was data cleaning. Coming from a sociology-influenced statistics background, we felt that it was important to account for skips, etc. from the survey instruments in a logical manner. We spent a great deal of time cleaning and recoding data to reflect implied answers from the survey structure, and force categorical responses to be treated as such. We also dropped ""administrative"" variables, like time of survey and sample weights, and any variables that were constant across traning observations. Once this was done, we used one of four methods to reduce the number of variables used in predictive modeling: LASSO, Bayesian GLM, Horseshoe, and Bayesian Additive Regression Trees (BART). All predictive models were fit using BART. We fit many combinations of variable selection methods and hyperparameter settings for BART, in order to explore which of these would be related to final predictive performance."
Yoshihiko Suhara,sy,"My approach was training Gradient-boosted Tree models on imputed features without feature selection, with intensive hyper-parameter search based on Grid Search. The hyperparameter candidates were manually crafted based on my experience in data science competitions. The approach was fully data-driven; I made the best use of computation resource for the hyper-parameter search to reduce the risk of overfitting, and I chose the predictive model that performed best in the cross-validation evaluation."
Ryan James Compton,rcompton,"Our method involved cleaning, balancing, and then splitting the data set to ensure a more generalizable model. Due to the high number of variables within the data set, we used Principal Component Analysis as a feature engineering method to reduce both the number of variables and redundant information. After conducting a parameter search for how many components would be best for each dependent variable, modeling was conducted through Cross Validation and Random Forests. The best model found (through MSE performance) would then be used to make predictions for the Challenge test set."
Stephen McKay,the_Brit,"Subject specific knowledge was used to identify a long list of those variables most likely to have associations with the outcomes of interest, including some of the scales available in the survey and values of the outcomes in earlier waves. Statistical measures (R-squared, MSE, regression coefficients) were then used, alongside subject expertise, to produce final models from among that list. Continuous outcomes were modeled using relatively small random forests, and binary outcomes using logistic regression."
Allison C. Morgan,amorgan,"We chose only ``constructed'' variables -- those derived from the originally collected data by domain experts -- to train our models on. These observations were more or less complete, meaning the issue of handling missing data was less relevant for us here. In processing our data, we maintained discrete categorical variables and turned continuous variables into discrete variables by binning them into quartiles. This binning was done for later use with a different ML approach that required categorical variables. In retrospect, it would have been wise to allow our models to learn the appropriate divisions for a continuous variable. We predicted all outcome variables using a combination of regularized linear regression (for continuous outcomes) and a random forest (binary outcomes). Results were evaluated based on our ranking on the leadership board at the time of submission."
Claudia V. Roberts,chicacvr,"We divided the project into two steps. In step 1, we used a completely automatic approach that does not consider the data (the norm in data mining) to fit 124 models for GPA prediction. In step 2, we attempt to improve upon our results. We use a strategy that combines engineering-centric statistical analysis techniques with classical, more manual social science methodologies: we examined each variable in the codebook, manually selecting the ones believed to be predictive of academic achievement based on a non-expert reading of domain-specific research. Results indicate that it in most cases, it pays off for engineers to “make friends” with the FFCWS codebooks. We were able to improve the predictive accuracy of 6 of the 10 top step 1 models, of which 4 saw significant improvements. However, manual variable selection did not improve the predictive ability of the 2 most accurate models from step 1. We tried many different approaches to data pre-processing. We tried almost all combinations of 4 different decisions: 2 types of automatic variable selection (F-test and mutual information) using 2 thresholds (10\% and 20\%), 2 types of imputation strategies (median and mode), and 2 standardization approaches (no standardization and standardization)."
Eaman Jahani,eaman,"I worked as an individual member of a group. The group generated a single pipeline for data cleaning, imputation, and variable transformation which we all used for our own independent statistical learning step. The group prediction was an ensemble of our individual model predictions. Our data pipeline first determined which variables are categorical and which are continuous based on the number of unique values they take. Then it converted all categorical variables to dummies, including missingness dummies. It also dropped variables with low variance or high rate of missingness. The final cleaned data had more than 20,000 features. So our individual models had to do aggressive feature selection. To reduce the number of features prior to model learning, I performed univariate feature selection and reduced the number of features to the top most predictive 100, 300, 1000 and 1500 features. Prior to model building and feature selection, I also added various transformations of the continuous variables, e.g. log or square or square-root, to the data matrix. I only attempted to predict the continuous variables. My submission used a multi task elastic net which predicted all dependent variables, GPA, grit and material hardship, at the same time. The multi-task elastic net could be more efficient in case there is significant correlation between the dependent variables. The grit prediction came from this multi task model. For GPA, I took the average of two models. The first model was an elastic net (trained only on GPA) using the top 1500 features selected first through univariate feature selection. The second model was a random forest regressor on top 1000 features. In both models, features were normalized. Elastic net was also trained on gpa-squared since this transformation of the dependent variable gave better results. For material hardship, I used an elastic net on the top 300 features."
Debanjan Datta and Brian J. Goode,bjgoode,"Our approach to the the Challenge was primarily focused on survey structure and variable construction. In the former, we recognized when either dropouts would occur or the information would not be available due to a previous answer. These data were then imputed using the nearest data point in terms of year, relation, or question type. In the latter strategy, variables were constructed by counting scale responses forming the Material Hardship, Grit, and GPA outcomes."
Erik H. Wang and Diana M. Stanescu,haixiaow,"Our approach consists of the following steps. First, we do early house-cleaning by dropping variables with more than 60 percent missing and dropping variables with standard deviation smaller than .01. Second, we mean-impute the data and perform LASSO regressions of the outcome variables on all remaining covariates. We then drop any covariate whose coefficient is zero. Third, for the remaining covariates, we identify their originals (i.e., before mean imputation), and apply multiple-imputation using Amelia (which employs EM algorithms). We apply LASSO again for variable-selection using the Amelia-imputed dataset. When applying Amelia, we set M = 5 and pick the third dataset."
Diana Mercado-Garcia,ddmmgg,NA